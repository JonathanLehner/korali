#include "modules/problem/bayesian/latent/hierarchicalLatent/hierarchicalLatent.hpp"

#include <math.h>
#include <gsl/gsl_matrix.h>
#include <gsl/gsl_vector.h>
#include <gsl/gsl_linalg.h>
#include <gsl/gsl_blas.h>



/* The problem initialization; here, create hyperparameter variables, add them to the problem's variable list,
    and set up index lists to find both types of variables in that list.*/
void korali::problem::bayesian::latent::HierarchicalLatent::initialize()
{

  korali::problem::bayesian::Latent::initialize();

  int _numberLatentVariables = _k->_variables.size();
  //  initializeGSLVariables(_numberLatentVariables);
  // _sDimension = numberLatentVariables(1 + numberLatentVariables);

  // Just for convenience and code readability. The hyperparameters will all have indices higher than those of the
  // latent variables, and cov-variables will come after mean-variables. These could all be removed to optimize runspeed.
 _latentVariableIndices.clear();
// _hyperparameterVariableIndices.clear();
// _hyperparameterMeanIndices.clear();
// _hyperparameterCovIndices.clear();
 _hyperparameterCovIndices.resize(_numberLatentVariables);
 for (size_t i = 0; i< _hyperparameterCovIndices.size(); i++)
   _hyperparameterCovIndices[i].clear();

 _normalLatentIndices.clear();
 _lognormalLatentIndices.clear();
 _logitnormalLatentIndices.clear();

// std::vector<korali::Variable*> _hyperparameters;
 _hyperparameterMean.clear();
 _hyperparameterCovariance.clear();

 for (size_t i = 0; i < _k->_variables.size(); i++)
 {
  // Check variables type
  std::string typeString = _k->_variables[i]->_bayesianType;
  if (typeString == "Latent")   { _latentVariableIndices.push_back(i); }
  else { if (typeString == "Latent")
            {_k->_logger->logError("Please only define Latent variables and their distribution type. Korali will automatically generate the hyperparameters. \n");}
         else _k->_logger->logError("Unrecognized Variable Type %s \n", typeString.c_str());   }

  /* Check distributions and sort latent variables by distribution */
  bool recognizedDistribution = false;
  std::string distribString = _k->_variables[i]->_latentVariableDistributionType;
  if (distribString == "Normal")   { _normalLatentIndices.push_back(i);   recognizedDistribution = true; }
  if (distribString == "Log-Normal")   { _lognormalLatentIndices.push_back(i);   recognizedDistribution = true; }
  if (distribString == "Logit-Normal")   { _logitnormalLatentIndices.push_back(i);   recognizedDistribution = true; }
  if (distribString == "NA")   {
    _latentVariableIndices.push_back(i);   recognizedDistribution = true;
    _k->_logger->logError("Each latent variable must be either normally, log-normally or logit-normally distributed. NA is only for internal use.", typeString);
   }
  if (recognizedDistribution == false) _k->_logger->logError("Unrecognized distribution type: %s.\n", distribString.c_str());
 }

 /*  Now, generate the hyperparameters:
    1. means - one mean per latent variable.
      Each mean is for whichever transformed version of the variable that is normally distributed. */
 for (size_t i = 0; i < _numberLatentVariables; i++){
   korali::Variable *new_hyperparam = new korali::Variable;
   new_hyperparam->_latentVariableDistributionType = "NA";
   new_hyperparam->_bayesianType = "Hyperparameter";
   new_hyperparam->_name = "Mean of "+_k->_variables[i]->_name;

    /* initialize variable's mean with the user-defined initial value: */
   std::string distribString = _k->_variables[i]->_latentVariableDistributionType.c_str();
   if (distribString == "Normal")
     new_hyperparam->_initialValue = _k->_variables[i]->_initialValue;
   else {
     if (distribString == "Log-Normal")
       new_hyperparam->_initialValue = log(_k->_variables[i]->_initialValue);
     else {
       if (distribString == "Logit-Normal"){
         double ival = _k->_variables[i]->_initialValue;
         if ((ival < 0) || (ival >= 1) )
           _k->_logger->logError("Initial value of logit-normally distributed variable needs to be in range [0, 1).");
         new_hyperparam->_initialValue = (ival / (1 - ival));
       }
       else
         _k->_logger->logError("Unrecognized 'Latent Variable Distribution' ");
     }
   }

   // Todo: Add any other parameters here that will be required by the hierarchical SAEM. Initial values? Bounds?

//  volatile int done = 0;
//  while (!done) sleep(1);

   _hyperparameterMean.push_back( new_hyperparam );
//   _hyperparameters.push_back( &new_hyperparam );
//   _hyperparameterMeanIndices.push_back(i + _k->_variables.size());
 }

 /* 2. covariance matrices - we use Cholesky decomposition to ensure positive definiteness.
      Note: If ever required, could add 'groups' of variables that share a covariance matrix, while groups are independent of each other. */
 //  Degrees of freedom of the covariance matrix: n*(n+1)/2; we over-represent it as matrix
 for (size_t i = 0; i < _numberLatentVariables; i++){
   for (size_t j = 0; j < _k->_variables.size(); j++){
     korali::Variable *new_hyperparam = new korali::Variable;
     new_hyperparam->_latentVariableDistributionType = "NA";
     new_hyperparam->_bayesianType = "Hyperparameter";
     new_hyperparam->_name = "Cov["+std::to_string(i)+", "+std::to_string(j)+"]";   // Cov[i][j]
     if (i == j) new_hyperparam->_initialValue = 5.;
     else        new_hyperparam->_initialValue = 0.;
     // Todo: Add any other parameters here that will be required by hierarchical SAEM. Initial values? Bounds?
   _hyperparameterCovariance.push_back( new_hyperparam );
   _hyperparameterCovIndices[i].push_back(j + i * _k->_variables.size() );
//   _hyperparameters.push_back( &new_hyperparam );
//   _hyperparameterCovIndices[i].push_back(j + i * _k->_variables.size() + 2 * _k->_variables.size());
   }
 }

// _k->_variables.insert( _k->_variables.end(), _hyperparameters.begin(), _hyperparameters.end());
// for (size_t i = 0; i < _hyperparameters.size(); i++ )
//   _hyperparameterVariableIndices.push_back(_numberLatentVariables + i);

}

///* De-allocate GSL matrices and vectors */
//korali::problem::bayesian::latent::HierarchicalLatent::~HierarchicalLatent(void){
//  gsl_matrix_free(covMatrixGSL);
//  gsl_matrix_free(chol);
//  gsl_vector_free(latent_minus_mean);
//  gsl_vector_free(x);
//}

//
///*! @brief Evaluate the sufficient statistics, sufficient for determining the hyperparameters.
//        Not necessarily equal to the 'sufficient statistics' of an exponential family distribution.
//        sample is expected to contain parameter 'Latent Variables' */
//void korali::problem::bayesian::latent::HierarchicalLatent::evaluateS(korali::Sample& sample){
//  //sample.run(_sOfLikelihoodModel); // TODO
//}

/*! @brief Evaluate the user-defined function giving the conditional log-likelihood of the data, given some
           values for the latent variables. See the description of this function for more information.
           (It will set "Conditional logLikelihood" and use "Latent Variables".) */
void korali::problem::bayesian::latent::HierarchicalLatent::evaluateConditionalLogLikelihood(korali::Sample& sample){
  sample.run(_conditionalLogLikelihoodFunction);
}

/*! @brief Evaluate the total loglikelihood of the data (if any), given values for latent variables and hyperparameters.
    @param sample: Is expected to contain the fields ["Latent Variables"], ["Mean"] and ["Covariance Matrix"]. Here, "Mean" should be
                    a vector (size := n), "Covariance Matrix" should be an nxn vector of vectors. (No special encoding of the cov.)
                    Todo: Refactor to use "Hyperparameters" instead of "Mean" and "Covariance .."?

         TODO: Move calculation of cholesky decomposition out of this function, and into hSAEM. Don't want to
                re-calculate this for every latent variable sample.
         TODO: But first, test it.
 */
void korali::problem::bayesian::latent::HierarchicalLatent::evaluateLogLikelihood(korali::Sample& sample)
{
//  volatile int done = 0;
//  while (!done) sleep(1);

  // calculate log[ p(latent | hyperparams) ]:
  // -- need a multivariate log-gaussian function for this
  // += conditional log-llh (log[p(data | latent)])


  auto latentVariables = sample["Latent Variables"].get<std::vector<double>>();
  auto mean = sample["Mean"].get<std::vector<double>>();
  int N = mean.size(); // Number of latent variables
  auto covMatrix = sample["Covariance Matrix"].get<std::vector<std::vector<double>>>();
  assert (N == covMatrix.size());
  for (std::vector<double> v : covMatrix)
    assert (v.size() == N);
//  gsl_matrix* covMatrixGSL = sample["Covariance Matrix"].get<gsl_matrix*>(); // todo: does this work? - No, need to pass a correct type for the pointer, instead of matrix*
//  assert (covMatrixGSL.size1 == N);
//  assert (covMatrixGSL.size2 == N);

  //gsl_matrix_const_view covMatrixGSL = gsl_matrix_const_view_array(&covMatrix[0][0], N, N); // This works, but for memcopy below we need a proper matrix
  gsl_matrix* covMatrixGSL = gsl_matrix_alloc(N, N);
  covMatrixGSL->data = &covMatrix[0][0]; // todo does this work?


  // First, calculate the cholesky decomposition, cov = L'*L
  gsl_matrix* chol = gsl_matrix_alloc(N, N);
  gsl_matrix_memcpy(chol, covMatrixGSL);
  int status = gsl_linalg_cholesky_decomp1(chol); //lower triangular part + diagonal of chol afterwards contain L from the cholesky decomposition cov = L*L'

  // --> log(det(covariance)) = sum(log( diag(L) ))
  double logdet = 0;
  for (size_t i = 0; i < N; i++){
    logdet += log(gsl_matrix_get(chol, i, i));
  }

  // Then, invert the covariance matrix using the cholesky decomposition cov = L'*L

  // for inversion/solving a linear eqation, could add a small positive diagonal matrix
//  double eps = 1.e-10;
//  gsl_matrix* epsMatrix = gsl_matrix_alloc(N, N);
//  gsl_matrix_set_identity(epsMatrix);
//  gsl_matrix_scale(epsMatrix, eps);

  //gsl_vector* b = gsl_vector_alloc(N);
  gsl_vector *latent_minus_mean  = gsl_vector_alloc(N);
  latent_minus_mean->data = &latentVariables[0];
  gsl_vector_const_view mean_view = gsl_vector_const_view_array(&mean[0], N);
  gsl_vector_sub(latent_minus_mean, &mean_view.vector); // todo: &... or just mean_view.vector?
  //std::vector<double> latent_minus_mean(latentVariables); // should copy -?
  //std::transform(latent_minus_mean.begin(), latent_minus_mean.end(), mean.begin(), latent_minus_mean.begin(), std::minus<double>());

  //gsl_vector_const_view b(&latent_minus_mean, N);
  gsl_vector* x = gsl_vector_alloc(N);
  gsl_linalg_cholesky_solve(chol, latent_minus_mean, x); // -> x = L^-1 * (latent - mean)

  // --> log(p( latent | mean, cov )) = || (latent - mean)*L^-1 ||^2 / 2
  //                                  = (latent - mean)^T * cov^-1 / 2 * (latent - mean)
  double lllh;
  gsl_blas_ddot (x, x, &lllh);
  lllh = - lllh / 2.;

  // add the log of the normalization factor:
  lllh -= 1./2. * (log(2*M_PI) + logdet);

  evaluateConditionalLogLikelihood(sample);
  //               log(p) =         log(p(y | latent))          + log(p(latent | hyperparams))
  sample["LogLikelihood"] = sample["Conditional LogLikelihood"].get<double>() + lllh;

  gsl_matrix_free(covMatrixGSL);
  gsl_matrix_free(chol);
  gsl_vector_free(latent_minus_mean);
  gsl_vector_free(x);
}

///* @brief allocate memory for all GSL-matrix and GSL-vector local variables used in calculating the loglikelihood
//    @param nLatent: The number of latent variables in this problem.  */
//void korali::problem::bayesian::latent::HierarchicalLatent::initializeGSLVariables(int nLatent){
//  covMatrixGSL = gsl_matrix_alloc(nLatent, nLatent);
//  chol = gsl_matrix_alloc(nLatent, nLatent);
//  latent_minus_mean = gsl_vector_alloc(nLatent);
//  x = gsl_vector_alloc(nLatent);
//}

/*! @brief Not implemented */
void korali::problem::bayesian::latent::HierarchicalLatent::evaluateLogLikelihoodGradient(korali::Sample& sample)
{
  _k->_logger->logError("Gradient not yet implemented for selected bayesian problem ('HierarchicalLatent') and log likelihood model.");
}

/*! @brief Not implemented */
void korali::problem::bayesian::latent::HierarchicalLatent::evaluateFisherInformation(korali::Sample& sample)
{
  _k->_logger->logError("Fisher information not yet implemented for selected bayesian problem ('HierarchicalLatent') and log likelihood model.");
}

