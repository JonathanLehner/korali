#include "modules/problem/bayesian/latent/hierarchicalLatent/hierarchicalLatent.hpp"

#include <math.h>
#include <gsl/gsl_matrix.h>
#include <gsl/gsl_vector.h>
#include <gsl/gsl_linalg.h>
#include <gsl/gsl_blas.h>


/* The problem initialization; here, create hyperparameter variables and set up index lists to
         find both types of variables */
void korali::problem::bayesian::latent::HierarchicalLatent::initialize()
{
  korali::problem::bayesian::Latent::initialize();

  // Just for convenience. The hyperparameters will all have indices higher than those of the latent variables
 _latentVariableIndices.clear();
 _hyperparameterVariableIndices.clear();

 _normalLatentIndices.clear();
 _lognormalLatentIndices.clear();
 _logitnormalLatentIndices.clear();

 std::vector<korali::Variable> _hyperparameters;
 _hyperparameters.clear();

 for (size_t i = 0; i < _k->_variables.size(); i++)
 {
  // Check variables type
  std::string typeString = _k->_variables[i]->_bayesianType;
  if (typeString == "Latent")   { _latentVariableIndices.push_back(i); }
  else { if (typeString == "Latent")
            {_k->_logger->logError("Please only define Latent variables and their distribution type. Korali will automatically generate the hyperparameters. \n");}
         else _k->_logger->logError("Unrecognized Variable Type %s \n", typeString.c_str());   }

  // Check distributions and sort latent variables by distribution
  bool recognizedDistribution = false;
  std::string distribString = _k->_variables[i]->_latentVariableDistributionType;
  if (distribString == "Normal")   { _normalLatentIndices.push_back(i);   recognizedDistribution = true; }
  if (distribString == "Log-Normal")   { _lognormalLatentIndices.push_back(i);   recognizedDistribution = true; }
  if (distribString == "Logit-Normal")   { _logitnormalLatentIndices.push_back(i);   recognizedDistribution = true; }
  if (distribString == "NA")   {
    _latentVariableIndices.push_back(i);   recognizedDistribution = true;
    _k->_logger->logError("Each latent variable must be either normally, log-normally or logit-normally distributed. NA is only for internal use.", typeString);
   }
  if (recognizedDistribution == false) _k->_logger->logError("Unrecognized distribution type: %s.\n", distribString.c_str());
 }

 // * Now, generate the hyperparameters:
 // 1. means - one mean per latent variable.
 //     Each mean is for whichever transformed version of the variable that is normally distributed.
 for (size_t i = 0; i < _k->_variables.size(); i++){
   korali::Variable new_hyperparam;
   new_hyperparam._latentVariableDistributionType = "NA";
   new_hyperparam._bayesianType = "Hyperparameter";
   new_hyperparam._name = "Mean of "+_k->_variables[i]->_name;
   // Todo: Add any other parameters here that will be required by the hierarchical SAEM. Initial values? Bounds?
 }

 // 2. covariance matrices - we use Cholesky decomposition to ensure positive definiteness.
 // Note: If ever required, could add 'groups' of variables that share a covariance matrix, while groups are independent of each other.
 // Degrees of freedom of the lower-triangular matrix L: n*(n+1)/2
 for (size_t i = 0; i < _k->_variables.size(); i++){
   for (size_t j = i; j < _k->_variables.size(); j++){
     korali::Variable new_hyperparam;
     new_hyperparam._latentVariableDistributionType = "NA";
     new_hyperparam._bayesianType = "Hyperparameter";
     new_hyperparam._name = "L["+std::to_string(i)+", "+std::to_string(j)+"] of Covariance LL'";   // L[i][j]
     // Todo: Add any other parameters here that will be required by the hierarchical SAEM. Initial values? Bounds?
   }
 }

}

void korali::problem::bayesian::latent::HierarchicalLatent::execute(korali::Sample& sample){
  evaluateLogLikelihood(sample);
}


/*! @brief Evaluate the sufficient statistics, sufficient for determining the hyperparameters.
        Not necessarily equal to the 'sufficient statistics' of an exponential family distribution.
        sample is expected to contain parameter 'Latent Variables' */
void korali::problem::bayesian::latent::HierarchicalLatent::evaluateS(korali::Sample& sample){
  //sample.run(_sOfLikelihoodModel); // TODO
}

/*! @brief Evaluate the user-defined function giving the conditional log-likelihood of the data, given some
           values for the latent variables. See the description of this function for more information.
           (It will set "Conditional logLikelihood" and use "Latent Variables".) */
void korali::problem::bayesian::latent::HierarchicalLatent::evaluateConditionalLogLikelihood(korali::Sample& sample){
  sample.run(_conditionalLogLikelihoodFunction);
}

/*! @brief Evaluate the total loglikelihood of the data (if any), given values for latent variables and hyperparameters.
    @param sample: Is expected to contain the fields ["Latent Variables"], ["Mean"] and ["Covariance Matrix"]. Here, "Mean" should be
                    a vector (size := n), "Covariance Matrix" should be an nxn vector of vectors. (No special encoding of the cov.)
                    Todo: Refactor to use "Hyperparameters" instead of "Mean" and "Covariance .."?

         TODO: Move calculation of cholesky decomposition out of this function, and into hSAEM. Don't want to
                re-calculate this for every latent variable sample.
         TODO: But first, test it.
 */
void korali::problem::bayesian::latent::HierarchicalLatent::evaluateLogLikelihood(korali::Sample& sample)
{
  // calculate log[ p(latent | hyperparams) ]:
  // -- need a multivariate log-gaussian function for this
  // += conditional log-llh (log[p(data | latent)])


  auto latentVariables = sample["Latent Variables"].get<std::vector<double>>();
  auto mean = sample["Mean"].get<std::vector<double>>();
  int N = mean.size(); // Number of latent variables
  auto covMatrix = sample["Covariance Matrix"].get<std::vector<std::vector<double>>>();
  assert (N == covMatrix.size());
  for (std::vector<double> v : covMatrix)
    assert (v.size() == N);
//  gsl_matrix* covMatrixGSL = sample["Covariance Matrix"].get<gsl_matrix*>(); // todo: does this work? - No, need to pass a correct type for the pointer, instead of matrix*
//  assert (covMatrixGSL.size1 == N);
//  assert (covMatrixGSL.size2 == N);

  //gsl_matrix_const_view covMatrixGSL = gsl_matrix_const_view_array(&covMatrix[0][0], N, N); // This works, but for memcopy below we need a proper matrix
  gsl_matrix* covMatrixGSL = gsl_matrix_alloc(N, N);
  covMatrixGSL->data = &covMatrix[0][0]; // todo does this work?


  // First, calculate the cholesky decomposition, cov = L'*L
  gsl_matrix* chol = gsl_matrix_alloc(N, N);
  gsl_matrix_memcpy(chol, covMatrixGSL);
  int status = gsl_linalg_cholesky_decomp1(chol); //lower triangular part + diagonal of chol afterwards contain L from the cholesky decomposition cov = L*L'

  // --> log(det(covariance)) = sum(log( diag(L) ))
  double logdet = 0;
  for (size_t i = 0; i < N; i++){
    logdet += log(gsl_matrix_get(chol, i, i));
  }

  // Then, invert the covariance matrix using the cholesky decomposition cov = L'*L

  // for inversion/solving a linear eqation, could add a small positive diagonal matrix
//  double eps = 1.e-10;
//  gsl_matrix* epsMatrix = gsl_matrix_alloc(N, N);
//  gsl_matrix_set_identity(epsMatrix);
//  gsl_matrix_scale(epsMatrix, eps);

  //gsl_vector* b = gsl_vector_alloc(N);
  gsl_vector *latent_minus_mean  = gsl_vector_alloc(N);
  latent_minus_mean->data = &latentVariables[0];
  gsl_vector_const_view mean_view = gsl_vector_const_view_array(&mean[0], N);
  gsl_vector_sub(latent_minus_mean, &mean_view.vector); // todo: &... or just mean_view.vector?
  //std::vector<double> latent_minus_mean(latentVariables); // should copy -?
  //std::transform(latent_minus_mean.begin(), latent_minus_mean.end(), mean.begin(), latent_minus_mean.begin(), std::minus<double>());

  //gsl_vector_const_view b(&latent_minus_mean, N);
  gsl_vector* x = gsl_vector_alloc(N);
  gsl_linalg_cholesky_solve(chol, latent_minus_mean, x); // -> x = L^-1 * (latent - mean)

  // --> log(p( latent | mean, cov )) = || (latent - mean)*L^-1 ||^2 / 2
  //                                  = (latent - mean)^T * cov^-1 / 2 * (latent - mean)
  double lllh;
  gsl_blas_ddot (x, x, &lllh);
  lllh = - lllh / 2.;

  // add the log of the normalization factor:
  lllh -= 1./2. * (log(2*M_PI) + logdet);

  evaluateConditionalLogLikelihood(sample);
  //               log(p) =         log(p(y | latent))          + log(p(latent | hyperparams))
  sample["LogLikelihood"] = sample["Conditional LogLikelihood"].get<double>() + lllh;


}

/*! @brief Not implemented */
void korali::problem::bayesian::latent::HierarchicalLatent::evaluateLogLikelihoodGradient(korali::Sample& sample)
{
  _k->_logger->logError("Gradient not yet implemented for selected bayesian problem ('HierarchicalLatent') and log likelihood model.");
}

/*! @brief Not implemented */
void korali::problem::bayesian::latent::HierarchicalLatent::evaluateFisherInformation(korali::Sample& sample)
{
  _k->_logger->logError("Fisher information not yet implemented for selected bayesian problem ('HierarchicalLatent') and log likelihood model.");
}
