#include "modules/problem/bayesian/latent/hierarchicalLatent/hierarchicalLatent.hpp"

#include <math.h>
#include <gsl/gsl_matrix.h>
#include <gsl/gsl_vector.h>
#include <gsl/gsl_linalg.h>
#include <gsl/gsl_blas.h>



/* The problem initialization; here, create hyperparameter variables, add them to the problem's variable list,
    and set up index lists to find both types of variables in that list.*/
void korali::problem::bayesian::latent::HierarchicalLatent::initialize()
{
  korali::problem::bayesian::Latent::initialize();

  // * helper variables
  int numberLatentVariables = _k->_variables.size();
    // store the distribution indices in this list, sorted:
  std::vector<int> _sortedDistribIndices;
    // store one variable per mean among the hyperparameters, to access name, etc. further down:
  std::map<int, korali::Variable> exampleLatentVariables;

  // * clear and resize vectors
    // Just for convenience and code readability. The hyperparameters will all have indices higher than those of the
    // latent variables, and cov-variables will come after mean-variables. This could (should?)  be removed to optimize speed.
 _latentVariableIndices.clear();
   // these are not just for convenience
 _normalLatentIndices.clear();
 _lognormalLatentIndices.clear();
 _logitnormalLatentIndices.clear();

 _hyperparameterCovIndices.resize(numberLatentVariables);
 for (size_t i = 0; i< _hyperparameterCovIndices.size(); i++)
   _hyperparameterCovIndices[i].clear();
 _hyperparameterMean.clear();
 _hyperparameterCovariance.clear();

  /* Check and assign distribution types (of the latent variables), etc.*/
 for (size_t i = 0; i < _k->_variables.size(); i++)
 {
  // Check variables type
  std::string typeString = _k->_variables[i]->_bayesianType;
  if (typeString == "Latent")   { _latentVariableIndices.push_back(i); }
  else { if (typeString == "Latent")
            {_k->_logger->logError("Please only define Latent variables and their distribution type. Korali will automatically generate the hyperparameters. \n");}
         else _k->_logger->logError("Unrecognized Variable Type %s \n", typeString.c_str());   }

  /* Check distributions and sort latent variables by distribution */
  bool recognizedDistribution = false;
  std::string distribString = _k->_variables[i]->_latentVariableDistributionType;
  if (distribString == "Normal")   { _normalLatentIndices.push_back(i);   recognizedDistribution = true; }
  if (distribString == "Log-Normal")   { _lognormalLatentIndices.push_back(i);   recognizedDistribution = true; }
  if (distribString == "Logit-Normal")   { _logitnormalLatentIndices.push_back(i);   recognizedDistribution = true; }
  if (distribString == "NA")   {
    _latentVariableIndices.push_back(i);   recognizedDistribution = true;
    _k->_logger->logError("Each latent variable must be either normally, log-normally or logit-normally distributed. NA is only for internal use.", typeString);
   }
  if (recognizedDistribution == false) _k->_logger->logError("Unrecognized distribution type: %s.\n", distribString.c_str());

  int distribIndex = _k->_variables[i]->_distributionIndex;
  int individualIndex = _k->_variables[i]->_individualIndex;
  if (_latentVariableDistributions.count(distribIndex) == 0){
    _latentVariableDistributions[distribIndex] = distribString;
    exampleLatentVariables[distribIndex] = _k->_variables[i];
  }
  else
    assert (_latentVariableDistributions[distribIndex][0] == distribString && "Error: Assigned two+ latent variables to the same distribution (same 'Distribution Index'), but declared different distribution types for both.");
 }

  /* Check the distribution indices and individual indices.
       For each distribution index, there should be the same number of individual-indices.
       Then create lists to keep track of them. */
 for(map<int,int>::iterator it = _latentVariableDistributions.begin(); it != _latentVariableDistributions.end(); ++it)
   _sortedDistribIndices.push_back(it->first);
 for(auto var : _k->_variables)
   _sortedIndividualIndices.push_back(var->_individualIndex);
 std::sort( _sortedDistribIndices.begin(), _sortedDistribIndices.end() );
 std::sort( _sortedIndividualIndices.begin(), _sortedIndividualIndices.end() );
 _sortedIndividualIndices.erase( unique( _sortedIndividualIndices.begin(), _sortedIndividualIndices.end() ), _sortedIndividualIndices.end() );
 numberHyperparameterMeans = _latentVariableDistributions.size();
 numberIndividuals = _sortedDistribIndices.size();
 for (size_t i = 0; i < numberIndividuals; i++){
   for (size_t j = 0; j < numberHyperparameterMeans; j++){
        // TODO: Check the indices
        //   might also want to create an index of the form variable_index[i][j] that returns the list of variable z_{i,j}
   }
 }

 /*  Now, generate the hyperparameters:
    1. Means - one mean per distinct 'Distribution Index'.
      Each mean is for whichever transformed version of the variable that is normally distributed. */
 for (size_t i = 0; i < numberHyperparameterMeans; i++){
   // We need a "new" here, unless there's a way to prevent a not new-ed korali::Variable from being deallocated right after this function.
   korali::Variable *new_hyperparam = new korali::Variable;
   new_hyperparam->_latentVariableDistributionType = "NA";
   new_hyperparam->_bayesianType = "Hyperparameter";
   new_hyperparam->_name = "Mean of" + exampleLatentVariables[i]->_name.c_str();
   new_hyperparam->_distributionIndex = _sortedDistribIndices[i];
   for (size_t j = 0; j < numberLatentVariables; j++){
     if _k->_variables[j]->_distributionIndex == new_hyperparam->_distributionIndex
       _latentToHyperparameterMapping[j] = i;
   }

    /* initialize variable's mean with the user-defined initial value: */
   std::string distribString = _latentVariableDistributions[i];
   if (distribString == "Normal")
     new_hyperparam->_initialValue = _k->_variables[i]->_initialValue;
   else {
     if (distribString == "Log-Normal")
       new_hyperparam->_initialValue = log(_k->_variables[i]->_initialValue);
     else {
       if (distribString == "Logit-Normal"){
         double ival = _k->_variables[i]->_initialValue;
         if ((ival < 0) || (ival >= 1) )
           _k->_logger->logError("Initial value of logit-normally distributed variable needs to be in range [0, 1).");
         new_hyperparam->_initialValue = (ival / (1 - ival));
       }
       else
         _k->_logger->logError("Unrecognized 'Latent Variable Distribution' ");
     }
   }

   // Todo: Add any other parameters here that will be required by the hierarchical SAEM. Initial values?-OK. Bounds?

//  volatile int done = 0;
//  while (!done) sleep(1);

   _hyperparameterMean.push_back( new_hyperparam );
 }

 /* 2. covariance matrices - we use Cholesky decomposition to ensure positive definiteness.
      Note: If ever required, could add 'groups' of variables that share a covariance matrix, while groups are independent of each other. */
 //  Degrees of freedom of the covariance matrix: n*(n+1)/2; we over-represent it as matrix
 for (size_t i = 0; i < numberHyperparameterMeans; i++){
   for (size_t j = 0; j < numberHyperparameterMeans; j++){
     korali::Variable *new_hyperparam = new korali::Variable;
     new_hyperparam->_latentVariableDistributionType = "NA";
     new_hyperparam->_bayesianType = "Hyperparameter";
     new_hyperparam->_name = "Cov["+std::to_string(i)+", "+std::to_string(j)+"]";   // Cov[i][j]
     if (i == j) new_hyperparam->_initialValue = 5.; // just choosing something large - this could become a configuration parameter
     else        new_hyperparam->_initialValue = 0.;
     // Todo: Add any other parameters here that will be required by hierarchical SAEM. Initial values? Bounds?
   _hyperparameterCovariance.push_back( new_hyperparam );
   _hyperparameterCovIndices[i].push_back(j + i * _k->_variables.size() ); // This will probably never be used, might drop it
   }
 }

// _k->_variables.insert( _k->_variables.end(), _hyperparameters.begin(), _hyperparameters.end());
// for (size_t i = 0; i < _hyperparameters.size(); i++ )
//   _hyperparameterVariableIndices.push_back(numberLatentVariables + i);

}



/* De-allocate both vectors with hyperparameter korali-variables
    -- Todo: Ask someone knowledgeable about a better way to deal with allocating these variables, not using "new" */
korali::problem::bayesian::latent::HierarchicalLatent::~HierarchicalLatent(void){
  for ( int i = 0; i < _hyperparameterMean.size(); i++ )
    delete _hyperparameterMean[i];
  _hyperparameterMean.clear();
  for ( int i = 0; i < _hyperparameterMean.size(); i++ )
    delete _hyperparameterCovariance[i];
  _hyperparameterCovariance.clear();
//  gsl_matrix_free(covMatrixGSL);
//  gsl_matrix_free(chol);
//  gsl_vector_free(latent_minus_mean);
//  gsl_vector_free(x);
}

//
///*! @brief Evaluate the sufficient statistics, sufficient for determining the hyperparameters.
//        Not necessarily equal to the 'sufficient statistics' of an exponential family distribution.
//        sample is expected to contain parameter 'Latent Variables' */
//void korali::problem::bayesian::latent::HierarchicalLatent::evaluateS(korali::Sample& sample){
//  //sample.run(_sOfLikelihoodModel); // TODO
//}

/*! @brief Evaluate the user-defined function giving the conditional log-likelihood of the data, given some
           values for the latent variables. See the description of this function for more information.
           (It will set "Conditional LogLikelihood" and use "Latent Variables".) */
void korali::problem::bayesian::latent::HierarchicalLatent::evaluateConditionalLoglikelihood(korali::Sample& sample){
  sample.run(_conditionalLogLikelihoodFunction);
}

/*! @brief Evaluate the total loglikelihood of the data (if any), given values for latent variables and hyperparameters.
    @param sample: Is expected to contain the fields ["Latent Variables"], ["Mean"] and ["Covariance Matrix"]. Here, "Mean" should be
                    a vector (size := n), "Covariance Matrix" should be an nxn vector of vectors. (No special encoding of the cov.)
                    Todo: Refactor to use "Hyperparameters" instead of "Mean" and "Covariance .."?

         TODO: Move calculation of cholesky decomposition out of this function, and into hSAEM. Don't want to
                re-calculate this for every latent variable sample.
         TODO: But first, test it.
 */
void korali::problem::bayesian::latent::HierarchicalLatent::evaluateLoglikelihood(korali::Sample& sample)
{
//  volatile int done = 0;
//  while (!done) sleep(1);

  // calculate log[ p(latent | hyperparams) ]:
  // -- need a multivariate log-gaussian function for this
  // += conditional log-llh (log[p(data | latent)])


  auto latentVariables = sample["Latent Variables"].get<std::vector<double>>();
  auto mean = sample["Mean"].get<std::vector<double>>();
  int N = mean.size(); // Number of latent variables
  auto covMatrix = sample["Covariance Matrix"].get<std::vector<std::vector<double>>>();
  assert (N == covMatrix.size());
  for (std::vector<double> v : covMatrix)
    assert (v.size() == N);
//  gsl_matrix* covMatrixGSL = sample["Covariance Matrix"].get<gsl_matrix*>(); // todo: does this work? - No, need to pass a correct type for the pointer, instead of matrix*
//  assert (covMatrixGSL.size1 == N);
//  assert (covMatrixGSL.size2 == N);

  //gsl_matrix_const_view covMatrixGSL = gsl_matrix_const_view_array(&covMatrix[0][0], N, N); // This works, but for memcopy below we need a proper matrix
  gsl_matrix* covMatrixGSL = gsl_matrix_alloc(N, N);
  covMatrixGSL->data = &covMatrix[0][0]; // todo does this work?


  // First, calculate the cholesky decomposition, cov = L'*L
  gsl_matrix* chol = gsl_matrix_alloc(N, N);
  gsl_matrix_memcpy(chol, covMatrixGSL);
  int status = gsl_linalg_cholesky_decomp1(chol); //lower triangular part + diagonal of chol afterwards contain L from the cholesky decomposition cov = L*L'

  // --> log(det(covariance)) = sum(log( diag(L) ))
  double logdet = 0;
  for (size_t i = 0; i < N; i++){
    logdet += log(gsl_matrix_get(chol, i, i));
  }

  // Then, invert the covariance matrix using the cholesky decomposition cov = L'*L

  // for inversion/solving a linear eqation, could add a small positive diagonal matrix
//  double eps = 1.e-10;
//  gsl_matrix* epsMatrix = gsl_matrix_alloc(N, N);
//  gsl_matrix_set_identity(epsMatrix);
//  gsl_matrix_scale(epsMatrix, eps);

  //gsl_vector* b = gsl_vector_alloc(N);
  gsl_vector *latent_minus_mean  = gsl_vector_alloc(N);
  latent_minus_mean->data = &latentVariables[0];
  gsl_vector_const_view mean_view = gsl_vector_const_view_array(&mean[0], N);
  gsl_vector_sub(latent_minus_mean, &mean_view.vector); // todo: &... or just mean_view.vector?
  //std::vector<double> latent_minus_mean(latentVariables); // should copy -?
  //std::transform(latent_minus_mean.begin(), latent_minus_mean.end(), mean.begin(), latent_minus_mean.begin(), std::minus<double>());

  //gsl_vector_const_view b(&latent_minus_mean, N);
  gsl_vector* x = gsl_vector_alloc(N);
  gsl_linalg_cholesky_solve(chol, latent_minus_mean, x); // -> x = L^-1 * (latent - mean)

  // --> log(p( latent | mean, cov )) = || (latent - mean)*L^-1 ||^2 / 2
  //                                  = (latent - mean)^T * cov^-1 / 2 * (latent - mean)
  double lllh;
  gsl_blas_ddot (x, x, &lllh);
  lllh = - lllh / 2.;

  // add the log of the normalization factor:
  lllh -= 1./2. * (log(2*M_PI) + logdet);

  evaluateConditionalLoglikelihood(sample);
  //               log(p) =         log(p(y | latent))          + log(p(latent | hyperparams))
  sample["LogLikelihood"] = sample["Conditional LogLikelihood"].get<double>() + lllh;

  gsl_matrix_free(covMatrixGSL);
  gsl_matrix_free(chol);
  gsl_vector_free(latent_minus_mean);
  gsl_vector_free(x);
}

///* @brief allocate memory for all GSL-matrix and GSL-vector local variables used in calculating the loglikelihood
//    @param nLatent: The number of latent variables in this problem.  */
//void korali::problem::bayesian::latent::HierarchicalLatent::initializeGSLVariables(int nLatent){
//  covMatrixGSL = gsl_matrix_alloc(nLatent, nLatent);
//  chol = gsl_matrix_alloc(nLatent, nLatent);
//  latent_minus_mean = gsl_vector_alloc(nLatent);
//  x = gsl_vector_alloc(nLatent);
//}

