#include "auxiliar/fs.hpp"
#include "engine.hpp"
#include "modules/solver/agent/agent.hpp"
#include "sample/sample.hpp"
#include <chrono>

@startNamespace

void @className::initialize()
{
  _variableCount = _k->_variables.size();

  // Getting problem pointer
  _problem = dynamic_cast<problem::ReinforcementLearning *>(_k->_problem);

  // Allocating and obtaining action bounds information
  _actionLowerBounds.resize(_problem->_actionVectorSize);
  _actionUpperBounds.resize(_problem->_actionVectorSize);

  for (size_t i = 0; i < _problem->_actionVectorSize; i++)
  {
    auto varIdx = _problem->_actionVectorIndexes[i];
    _actionLowerBounds[i] = _k->_variables[varIdx]->_lowerBound;
    _actionUpperBounds[i] = _k->_variables[varIdx]->_upperBound;

    if (_actionUpperBounds[i] - _actionLowerBounds[i] <= 0.0) KORALI_LOG_ERROR("Upper (%f) and Lower Bound (%f) of action variable %lu invalid.\n", _actionUpperBounds[i], _actionLowerBounds[i], i);
  }

  // Initializing selected policy
  initializeAgent();

  // Initializing random seed for the shuffle operation
  mt = new std::mt19937(rd());
  mt->seed(_k->_randomSeed++);

  //  Pre-allocating space for the experience replay memory
  _stateVector.resize(_experienceReplayMaximumSize);
  _actionVector.resize(_experienceReplayMaximumSize);
  _rewardVector.resize(_experienceReplayMaximumSize);
  _truncatedStateVector.resize(_experienceReplayMaximumSize);
  _terminationVector.resize(_experienceReplayMaximumSize);
  _episodePosVector.resize(_experienceReplayMaximumSize);
  _episodeIdVector.resize(_experienceReplayMaximumSize);
  
  // ES specifics
  _policySampleVector.resize(_episodesPerUpdate);
  _sampleVector.resize(_episodesPerUpdate);
  _rewardSampleVector.resize(_episodesPerUpdate);
  _rewardSampleMean = 0.0;
  _rewardSampleSdev = 0.0;
  _rewardSampleMin = +Inf;
  _rewardSampleMax = -Inf;

  //  Pre-allocating space for state time sequence
  _stateTimeSequence.resize(_timeSequenceLength);

  /*********************************************************************
   *   // If initial generation, set initial agent configuration
   *********************************************************************/

  if (_k->_currentGeneration == 0)
  {
    _currentEpisode = 0;
    _policyUpdateCount = 0;
    _testingCandidateCount = 0;
    _currentSampleID = 0;
    _experienceCount = 0;

    // Initializing training and episode statistics
    _testingAverageReward = -korali::Inf;
    _testingStdevReward = +korali::Inf;
    _testingBestReward = -korali::Inf;
    _testingWorstReward = +korali::Inf;
    _trainingBestReward = -korali::Inf;
    _trainingBestEpisodeId = 0;
    _trainingAverageReward = 0.0f;
    _testingPreviousAverageReward = -korali::Inf;
    _testingBestAverageReward = -korali::Inf;
    _testingBestEpisodeId = 0;

    // Initializing REFER information

    // If cutoff scale is not defined, use a heuristic value
    if (_experienceReplayOffPolicyCutoffScale < 0.0f)
      KORALI_LOG_ERROR("Expericne Replay Cutoff Scale must be larger 0.0");

    _experienceReplayOffPolicyCount = 0;
    _experienceReplayOffPolicyRatio = 0.0f;
    _experienceReplayOffPolicyCurrentCutoff = _experienceReplayOffPolicyCutoffScale;
    _currentLearningRate = _learningRate;

    // Rescaling information
    _stateRescalingMeans = std::vector<float>(_problem->_stateVectorSize, 0.0);
    _stateRescalingSigmas = std::vector<float>(_problem->_stateVectorSize, 1.0);

    _rewardRescalingMean = 0.0f;
    _rewardRescalingSigma = 1.0f;
    _rewardRescalingCount = 0;
    _rewardOutboundPenalizationCount = 0;
  }

  // If this continues a previous training run, deserialize previous input experience replay
  if (_k->_currentGeneration > 0)
    if (_mode == "Training" || _testingBestPolicy.empty())
      deserializeExperienceReplay();

  // Getting agent's initial policy
  _trainingCurrentPolicy = getAgentPolicy();

  // Reset
  auto policy = _trainingCurrentPolicy["Policy"].get<std::vector<float>>();
  std::fill(policy.begin(), policy.end(), 0.0);
  _trainingCurrentPolicy["Policy"] = policy;
  setAgentPolicy(_trainingCurrentPolicy);

  // Initializing session-wise profiling timers
  _sessionRunningTime = 0.0;
  _sessionSerializationTime = 0.0;
  _sessionAgentComputationTime = 0.0;
  _sessionAgentCommunicationTime = 0.0;
  _sessionAgentPolicyEvaluationTime = 0.0;
  _sessionPolicyUpdateTime = 0.0;
  _sessionAgentAttendingTime = 0.0;

  // Initializing session-specific counters
  _sessionExperienceCount = 0;
  _sessionEpisodeCount = 0;
  _sessionGeneration = 1;
  _sessionPolicyUpdateCount = 0;

  // Calculating how many more experiences do we need in this session to reach the starting size
  _sessionExperiencesUntilStartSize = _stateVector.size() > _experienceReplayStartSize ? 0 : _experienceReplayStartSize - _stateVector.size();

  if (_mode == "Training")
  {
    // Creating storate for _agents and their status
    _agents.resize(_agentCount);
    _isAgentRunning.resize(_agentCount, false);
    _rolloutVector.resize(_agentCount);
  }

  if (_mode == "Testing")
  {
    // Fixing termination criteria for testing mode
    _maxGenerations = _k->_currentGeneration + 1;

    // Setting testing policy to best testing hyperparameters if not custom-set by the user
    if (_testingPolicy.empty())
    {
      // Checking if testing policies have been generated
      if (_testingBestPolicy.empty())
      {
        _k->_logger->logWarning("Minimal", "Trying to test policy, but no testing policies have been generated during training yet or given in the configuration. Using current training policy instead.\n");
        _testingPolicy = _trainingCurrentPolicy;
      }
      else
      {
        _testingPolicy = _testingBestPolicy;
      }
    }

    // Checking if there's testing samples defined
    if (_testingSampleIds.size() == 0)
      KORALI_LOG_ERROR("For testing, you need to indicate the sample ids to run in the ['Testing']['Sample Ids'] field.\n");

    // Prepare storage for rewards from tested samples
    _testingReward.resize(_testingSampleIds.size());
  }
}

void @className::runGeneration()
{
  if (_mode == "Training") trainingGeneration();
  if (_mode == "Testing") testingGeneration();
}

void @className::trainingGeneration()
{
  auto beginTime = std::chrono::steady_clock::now(); // Profiling

  // Setting generation-specific timers
  _generationRunningTime = 0.0;
  _generationSerializationTime = 0.0;
  _generationAgentComputationTime = 0.0;
  _generationAgentCommunicationTime = 0.0;
  _generationAgentPolicyEvaluationTime = 0.0;
  _generationPolicyUpdateTime = 0.0;
  _generationAgentAttendingTime = 0.0;
  
  auto policy = _trainingCurrentPolicy["Policy"].get<std::vector<float>>();
  size_t hyperparameterCount = policy.size();
  //for(size_t i = 0; i < hyperparameterCount; ++i) printf("t(%zu/%zu): %f\n", i, policy.size(), policy[i]);
  
  for(size_t rollout = 0; rollout < _episodesPerUpdate; rollout+=2)
  {
    _policySampleVector[rollout] = policy;
    _policySampleVector[rollout+1] = policy;
    _sampleVector[rollout].resize(hyperparameterCount);
    _sampleVector[rollout+1].resize(hyperparameterCount);
    for(size_t i = 0; i < hyperparameterCount; ++i)
    {
      _sampleVector[rollout][i] = _normalGenerator->getRandomNumber();
      _policySampleVector[rollout][i] += _noiseParameter*_sampleVector[rollout][i];
    }
    for(size_t i = 0; i < hyperparameterCount; ++i)
    {
      _sampleVector[rollout+1][i] = -_sampleVector[rollout][i];
      _policySampleVector[rollout+1][i] += _noiseParameter*_sampleVector[rollout+1][i];
    }

  }
  
  size_t rollout = 0;
  bool agentsRemain = false;
  // Running until all _agents have finished
  do
  {
    agentsRemain = false;

    // Launching (or re-launching) agents
    for (size_t agentId = 0; agentId < _agentCount; agentId++)
      if (_isAgentRunning[agentId] == false && rollout < _episodesPerUpdate)
      {
        knlohmann::json policy;
        policy["Policy"] = _policySampleVector[rollout];
        _agents[agentId]["Sample Id"] = _currentEpisode++;
        _agents[agentId]["Module"] = "Problem";
        _agents[agentId]["Operation"] = "Run Training Episode";
        _agents[agentId]["Policy Hyperparameters"] = policy;
        _agents[agentId]["State Rescaling"]["Means"] = _stateRescalingMeans;
        _agents[agentId]["State Rescaling"]["Standard Deviations"] = _stateRescalingSigmas;

        KORALI_START(_agents[agentId]);
        _rolloutVector[agentId] = rollout;
        rollout++;
        _isAgentRunning[agentId] = true;
      }

    // Listening to _agents for incoming experiences
    KORALI_LISTEN(_agents);

    // Attending to running agents, checking if any experience has been received
    for (size_t agentId = 0; agentId < _agentCount; agentId++)
      if (_isAgentRunning[agentId] == true)
      {
        attendAgent(agentId);
        agentsRemain = true;
      }

    if (agentsRemain) KORALI_LISTEN(_agents);
  } while (rollout < _episodesPerUpdate || agentsRemain == true);

  _rewardSampleMean = 0.0;
  _rewardSampleMin = +Inf;
  _rewardSampleMax = -Inf;

  float rewardSampleMeanSquared = 0.0;
  for(size_t m = 0; m < _episodesPerUpdate; m++)
  {
    _rewardSampleMean += _rewardSampleVector[m];
    rewardSampleMeanSquared += _rewardSampleVector[m]*_rewardSampleVector[m];
    if(_rewardSampleVector[m] < _rewardSampleMin) _rewardSampleMin = _rewardSampleVector[m];
    if(_rewardSampleVector[m] > _rewardSampleMax) _rewardSampleMax = _rewardSampleVector[m];
  }
  _rewardSampleMean /= (float) _episodesPerUpdate;
  rewardSampleMeanSquared /= (float) _episodesPerUpdate;
  
  _rewardSampleSdev = std::sqrt(rewardSampleMeanSquared - _rewardSampleMean*_rewardSampleMean);

  // Update policy
  if(_experienceCount > _experienceReplayStartSize)
  {
    if(_policyUpdateCount == 0) rescaleStates();
    trainPolicy();
  }

  // Set new policy
  _trainingCurrentPolicy = getAgentPolicy();

  // Now serializing experience replay database
  if (_experienceReplaySerialize == true)
    if (_k->_fileOutputEnabled)
      if (_k->_fileOutputFrequency > 0)
        if (_k->_currentGeneration % _k->_fileOutputFrequency == 0)
          serializeExperienceReplay();

  // Measuring generation time
  auto endTime = std::chrono::steady_clock::now();                                                             // Profiling
  _sessionRunningTime += std::chrono::duration_cast<std::chrono::nanoseconds>(endTime - beginTime).count();    // Profiling
  _generationRunningTime += std::chrono::duration_cast<std::chrono::nanoseconds>(endTime - beginTime).count(); // Profiling

  /*********************************************************************
   * Updating statistics/bookkeeping
   *********************************************************************/

  // Updating average cumulative reward statistics
  _trainingAverageReward = 0.0f;
  ssize_t startEpisodeId = _trainingRewardHistory.size() - _trainingAverageDepth;
  ssize_t endEpisodeId = _trainingRewardHistory.size() - 1;
  if (startEpisodeId < 0) startEpisodeId = 0;
  for (ssize_t e = startEpisodeId; e <= endEpisodeId; e++)
    _trainingAverageReward += _trainingRewardHistory[e];
  _trainingAverageReward /= (float)(endEpisodeId - startEpisodeId + 1);

  // Increasing session's generation count
  _sessionGeneration++;
}

void @className::testingGeneration()
{
  // Allocating testing agents
  std::vector<Sample> testingAgents(_testingSampleIds.size());

  // Launching  agents
  for (size_t agentId = 0; agentId < _testingSampleIds.size(); agentId++)
  {
    testingAgents[agentId]["Sample Id"] = _testingSampleIds[agentId];
    testingAgents[agentId]["Module"] = "Problem";
    testingAgents[agentId]["Operation"] = "Run Testing Episode";
    testingAgents[agentId]["Policy Hyperparameters"] = _testingPolicy;
    testingAgents[agentId]["State Rescaling"]["Means"] = _stateRescalingMeans;
    testingAgents[agentId]["State Rescaling"]["Standard Deviations"] = _stateRescalingSigmas;

    KORALI_START(testingAgents[agentId]);
  }

  KORALI_WAITALL(testingAgents);

  for (size_t agentId = 0; agentId < _testingSampleIds.size(); agentId++)
    _testingReward[agentId] = testingAgents[agentId]["Testing Reward"].get<float>();
}

void @className::rescaleStates()
{
  // Calculation of state moments
  std::vector<float> sumStates(_problem->_stateVectorSize, 0.0);
  std::vector<float> squaredSumStates(_problem->_stateVectorSize, 0.0);

  for (size_t i = 0; i < _stateVector.size(); ++i)
    for (size_t d = 0; d < _problem->_stateVectorSize; ++d)
    {
      sumStates[d] += _stateVector[i][d];
      squaredSumStates[d] += _stateVector[i][d] * _stateVector[i][d];
    }

  _k->_logger->logInfo("Normal", " + Using State Normalization N(Mean, Sigma):\n");

  for (size_t d = 0; d < _problem->_stateVectorSize; ++d)
  {
    _stateRescalingMeans[d] = sumStates[d] / (float)_stateVector.size();
    if (std::isfinite(_stateRescalingMeans[d]) == false) _stateRescalingMeans[d] = 0.0f;

    _stateRescalingSigmas[d] = std::sqrt(squaredSumStates[d] / (float)_stateVector.size() - _stateRescalingMeans[d] * _stateRescalingMeans[d]);
    if (std::isfinite(_stateRescalingSigmas[d]) == false) _stateRescalingSigmas[d] = 1.0f;
    if (_stateRescalingSigmas[d] <= 1e-9) _stateRescalingSigmas[d] = 1.0f;

    _k->_logger->logInfo("Normal", " + State [%zu]: N(%f, %f)\n", d, _stateRescalingMeans[d], _stateRescalingSigmas[d]);
  }

  // Actual rescaling of initial states
  for (size_t i = 0; i < _stateVector.size(); ++i)
    for (size_t d = 0; d < _problem->_stateVectorSize; ++d)
      _stateVector[i][d] = (_stateVector[i][d] - _stateRescalingMeans[d]) / _stateRescalingSigmas[d];
}

void @className::calculateRewardRescalingFactors()
{
  float sumReward = 0.0;
  float sumSquareReward = 0.0;

  // Calculate mean and standard deviation of unscaled rewards.
  for (size_t i = 0; i < _rewardVector.size(); i++)
  {
    float reward = _rewardVector[i];
    sumReward += reward;
    sumSquareReward += reward * reward;
  }

  // Calculating reward scaling s,t. mean equals 0.0 and standard deviation 1.0.
  _rewardRescalingMean = sumReward / (float)_rewardVector.size();
  _rewardRescalingSigma = std::sqrt(sumSquareReward / (float)_rewardVector.size() - _rewardRescalingMean * _rewardRescalingMean + 1e-9);
  _rewardRescalingCount++;
}

void @className::attendAgent(size_t agentId)
{
  auto beginTime = std::chrono::steady_clock::now(); // Profiling

  // Storage for the incoming message
  knlohmann::json message;

  // Retrieving the experience, if any has arrived for the current agent.
  if (_agents[agentId].retrievePendingMessage(message))
  {
    // Getting episode Id
    size_t episodeId = message["Sample Id"];

    // If agent requested new policy, send the new hyperparameters
    if (message["Action"] == "Request New Policy")
      KORALI_SEND_MSG_TO_SAMPLE(_agents[agentId], _trainingCurrentPolicy);

    if (message["Action"] == "Send Episode")
    {
      //  Now that we have the entire episode, process its experiences (add them to replay memory)
      processEpisode(episodeId, message["Experiences"]);

      // Increasing total experience counters
      _experienceCount += message["Experiences"].size();
      _sessionExperienceCount += message["Experiences"].size();

      // Waiting for the agent to come back with all the information
      KORALI_WAIT(_agents[agentId]);

      // Getting the training reward of the latest episode
      _trainingLastReward = _agents[agentId]["Training Reward"].get<float>();

      // Storing bookkeeping information
      _trainingRewardHistory.push_back(_trainingLastReward);
      _trainingExperienceHistory.push_back(message["Experiences"].size());

      // ES speicifcs
      size_t rollout = _rolloutVector[agentId];
      _rewardSampleVector[rollout] = _trainingLastReward;

      // Keeping training statistics. Updating if exceeded best training policy so far.
      if (_trainingLastReward > _trainingBestReward)
      {
        _trainingBestReward = _trainingLastReward;
        _trainingBestEpisodeId = episodeId;
        _trainingBestPolicy = _agents[agentId]["Policy Hyperparameters"];
      }

      // If the policy has exceeded the threshold during training, we gather its statistics
      if (_agents[agentId]["Tested Policy"] == true)
      {
        _testingCandidateCount++;

        _testingPreviousAverageReward = _testingAverageReward;
        _testingAverageReward = _agents[agentId]["Average Testing Reward"].get<float>();
        _testingStdevReward = _agents[agentId]["Stdev Testing Reward"].get<float>();
        _testingBestReward = _agents[agentId]["Best Testing Reward"].get<float>();
        _testingWorstReward = _agents[agentId]["Worst Testing Reward"].get<float>();

        // If the average testing reward is better than the previous best, replace it
        // and store hyperparameters as best so far.
        if (_testingAverageReward > _testingBestAverageReward)
        {
          _testingBestAverageReward = _testingAverageReward;
          _testingBestEpisodeId = episodeId;
          _testingBestPolicy = _agents[agentId]["Policy Hyperparameters"];
        }
      }

      // Obtaining profiling information
      _sessionAgentComputationTime += _agents[agentId]["Computation Time"].get<double>();
      _sessionAgentCommunicationTime += _agents[agentId]["Communication Time"].get<double>();
      _sessionAgentPolicyEvaluationTime += _agents[agentId]["Policy Evaluation Time"].get<double>();
      _generationAgentComputationTime += _agents[agentId]["Computation Time"].get<double>();
      _generationAgentCommunicationTime += _agents[agentId]["Communication Time"].get<double>();
      _generationAgentPolicyEvaluationTime += _agents[agentId]["Policy Evaluation Time"].get<double>();

      // Set agent as finished
      _isAgentRunning[agentId] = false;

      // Increasing session episode count
      _sessionEpisodeCount++;
    }
  }

  auto endTime = std::chrono::steady_clock::now();                                                                    // Profiling
  _sessionAgentAttendingTime += std::chrono::duration_cast<std::chrono::nanoseconds>(endTime - beginTime).count();    // Profiling
  _generationAgentAttendingTime += std::chrono::duration_cast<std::chrono::nanoseconds>(endTime - beginTime).count(); // Profiling
}

void @className::processEpisode(size_t episodeId, knlohmann::json &episode)
{
  /*********************************************************************
  * Adding episode's experiences into the replay memory
  *********************************************************************/

  // Storage for the episode's cumulative reward
  float cumulativeReward = 0.0f;

  for (size_t expId = 0; expId < episode.size(); expId++)
  {
    // Getting state
    _stateVector.add(episode[expId]["State"].get<std::vector<float>>());

    // Getting action
    const auto action = episode[expId]["Action"].get<std::vector<float>>();
    _actionVector.add(action);

    // Getting reward
    float reward = episode[expId]["Reward"].get<float>();

    // If the action is outside the boundary, applying penalization factor
    if (_rewardOutboundPenalizationEnabled == true)
    {
      bool outOfBounds = false;
      for (size_t i = 0; i < _problem->_actionVectorSize; i++)
      {
        if (action[i] > _actionUpperBounds[i]) outOfBounds = true;
        if (action[i] < _actionLowerBounds[i]) outOfBounds = true;
      }

      if (outOfBounds == true)
      {
        reward = reward * _rewardOutboundPenalizationFactor;
        _rewardOutboundPenalizationCount++;
      }
    }

    _rewardVector.add(reward);

    // Keeping statistics
    cumulativeReward += reward;

    // Checking experience termination status and truncated state
    termination_t termination;
    std::vector<float> truncatedState;

    if (episode[expId]["Termination"] == "Non Terminal") termination = e_nonTerminal;
    if (episode[expId]["Termination"] == "Terminal") termination = e_terminal;
    if (episode[expId]["Termination"] == "Truncated")
    {
      termination = e_truncated;
      truncatedState = episode[expId]["Truncated State"].get<std::vector<float>>();
    }

    _terminationVector.add(termination);
    _truncatedStateVector.add(truncatedState);

    // Getting policy information and state value
    policy_t expPolicy;
    float stateValue;

    if (isDefined(episode[expId], "Policy", "State Value"))
    {
      expPolicy.stateValue = episode[expId]["Policy"]["State Value"].get<float>();
      stateValue = episode[expId]["Policy"]["State Value"].get<float>();
    }
    else
    {
      KORALI_LOG_ERROR("Policy has not produced state value for the current experience.\n");
    }

    if (isDefined(episode[expId], "Policy", "Distribution Parameters"))
      expPolicy.distributionParameters = episode[expId]["Policy"]["Distribution Parameters"].get<std::vector<float>>();

    if (isDefined(episode[expId], "Policy", "Action Index"))
      expPolicy.actionIndex = episode[expId]["Policy"]["Action Index"].get<size_t>();

    // Storing Episode information
    _episodeIdVector.add(episodeId);
    _episodePosVector.add(expId);
  }

}

std::vector<size_t> @className::generateMiniBatch(size_t miniBatchSize)
{
  // Allocating storage for mini batch experiecne indexes
  std::vector<size_t> miniBatch(miniBatchSize);

  for (size_t i = 0; i < miniBatchSize; i++)
  {
    // Producing random (uniform) number for the selection of the experience
    float x = _uniformGenerator->getRandomNumber();

    // Selecting experience
    size_t expId = std::floor(x * (float)(_stateVector.size() - 1));

    // Setting experience
    miniBatch[i] = expId;
  }

  // Sorting minibatch -- this helps with locality and also
  // to quickly detect duplicates when updating metadata
  std::sort(miniBatch.begin(), miniBatch.end());

  // Returning generated minibatch
  return miniBatch;
}

size_t @className::getTimeSequenceStartExpId(size_t expId)
{
  size_t startId = expId;

  // Adding (tmax-1) time sequences to the given experience
  for (size_t t = 0; t < _timeSequenceLength - 1; t++)
  {
    // If we reached the start of the ER, this is the starting episode in the sequence
    if (startId == 0) break;

    // Now going back one experience
    startId--;

    // If we reached the end of the previous episode, then add one (this covers the case where the provided experience is also terminal) and break.
    if (_terminationVector[startId] != e_nonTerminal)
    {
      startId++;
      break;
    }
  }

  return startId;
}

void @className::resetTimeSequence()
{
  _stateTimeSequence.clear();
}

std::vector<std::vector<std::vector<float>>> @className::getMiniBatchStateSequence(const std::vector<size_t> &miniBatch, const bool includeAction)
{
  // Getting mini batch size
  const size_t miniBatchSize = miniBatch.size();

  // Allocating state sequence vector
  std::vector<std::vector<std::vector<float>>> stateSequence(miniBatchSize);

  // Calculating size of state vector
  const size_t stateSize = includeAction ? _problem->_stateVectorSize + _problem->_actionVectorSize : _problem->_stateVectorSize;

#pragma omp parallel for
  for (size_t b = 0; b < miniBatch.size(); b++)
  {
    // Getting current expId
    const size_t expId = miniBatch[b];

    // Getting starting expId
    const size_t startId = getTimeSequenceStartExpId(expId);

    // Calculating time sequence length
    const size_t T = expId - startId + 1;

    // Resizing state sequence vector to the correct time sequence length
    stateSequence[b].resize(T);

    // Now adding states (and actions, if required)
    for (size_t t = 0; t < T; t++)
    {
      size_t curId = startId + t;
      stateSequence[b][t].reserve(stateSize);
      stateSequence[b][t].insert(stateSequence[b][t].begin(), _stateVector[curId].begin(), _stateVector[curId].end());
      if (includeAction) stateSequence[b][t].insert(stateSequence[b][t].begin(), _actionVector[curId].begin(), _actionVector[curId].end());
    }
  }

  return stateSequence;
}

std::vector<std::vector<float>> @className::getTruncatedStateSequence(size_t expId)
{
  // Getting starting expId
  size_t startId = getTimeSequenceStartExpId(expId);

  // Creating storage for the time sequence
  std::vector<std::vector<float>> timeSequence;

  // Now adding states, except for the initial one
  for (size_t e = startId + 1; e <= expId; e++)
    timeSequence.push_back(_stateVector[e]);

  // Lastly, adding truncated state
  timeSequence.push_back(_truncatedStateVector[expId]);

  return timeSequence;
}

void @className::finalize()
{
  if (_mode != "Training") return;

  if (_experienceReplaySerialize == true)
    if (_k->_fileOutputEnabled)
      serializeExperienceReplay();

  _k->_logger->logInfo("Normal", "Waiting for pending agents to finish...\n");

  // Waiting for pending agents to finish
  bool agentsRemain = true;
  do
  {
    agentsRemain = false;
    for (size_t agentId = 0; agentId < _agentCount; agentId++)
      if (_isAgentRunning[agentId] == true)
      {
        attendAgent(agentId);
        agentsRemain = true;
      }

    if (agentsRemain) KORALI_LISTEN(_agents);
  } while (agentsRemain == true);
}

void @className::serializeExperienceReplay()
{
  _k->_logger->logInfo("Detailed", "Serializing Agent's Training State...\n");
  auto beginTime = std::chrono::steady_clock::now(); // Profiling

  // Creating JSON storage variable
  knlohmann::json stateJson;

  // Serializing agent's database into the JSON storage
  for (size_t i = 0; i < _stateVector.size(); i++)
  {
    stateJson["Experience Replay"][i]["Episode Id"] = _episodeIdVector[i];
    stateJson["Experience Replay"][i]["Episode Pos"] = _episodePosVector[i];
    stateJson["Experience Replay"][i]["State"] = _stateVector[i];
    stateJson["Experience Replay"][i]["Action"] = _actionVector[i];
    stateJson["Experience Replay"][i]["Reward"] = _rewardVector[i];
    stateJson["Experience Replay"][i]["Truncated State"] = _truncatedStateVector[i];
    stateJson["Experience Replay"][i]["Termination"] = _terminationVector[i];
  }

  // Storing training/testing policies
  stateJson["Training"]["Current Policy"] = _trainingCurrentPolicy;
  stateJson["Training"]["Best Policy"] = _trainingBestPolicy;
  stateJson["Testing"]["Best Policy"] = _testingBestPolicy;

  // If results directory doesn't exist, create it
  if (!dirExists(_k->_fileOutputPath)) mkdir(_k->_fileOutputPath);

  // Resolving file path
  std::string statePath = _k->_fileOutputPath + "/state.json";

  // Storing database to file
  if (saveJsonToFile(statePath.c_str(), stateJson) != 0)
    KORALI_LOG_ERROR("Could not serialize training state into file %s\n", statePath.c_str());

  auto endTime = std::chrono::steady_clock::now();                                                                   // Profiling
  _sessionSerializationTime += std::chrono::duration_cast<std::chrono::nanoseconds>(endTime - beginTime).count();    // Profiling
  _generationSerializationTime += std::chrono::duration_cast<std::chrono::nanoseconds>(endTime - beginTime).count(); // Profiling
}

void @className::deserializeExperienceReplay()
{
  auto beginTime = std::chrono::steady_clock::now(); // Profiling

  // Creating JSON storage variable
  knlohmann::json stateJson;

  // Resolving file path
  std::string statePath = _k->_fileOutputPath + "/state.json";

  // Loading database from file
  _k->_logger->logInfo("Detailed", "Loading previous run training state from file %s...\n", statePath.c_str());
  if (loadJsonFromFile(stateJson, statePath.c_str()) == false)
    KORALI_LOG_ERROR("Trying to resume training or test policy but could not find or deserialize agent's state from from file %s...\n", statePath.c_str());

  // Clearing existing database
  _stateVector.clear();
  _actionVector.clear();
  _rewardVector.clear();
  _terminationVector.clear();
  _episodePosVector.clear();
  _episodeIdVector.clear();

  // Deserializing database from JSON to the agent's state
  for (size_t i = 0; i < stateJson["Experience Replay"].size(); i++)
  {
    _episodeIdVector.add(stateJson["Experience Replay"][i]["Episode Id"].get<size_t>());
    _episodePosVector.add(stateJson["Experience Replay"][i]["Episode Pos"].get<size_t>());
    _stateVector.add(stateJson["Experience Replay"][i]["State"].get<std::vector<float>>());
    _actionVector.add(stateJson["Experience Replay"][i]["Action"].get<std::vector<float>>());
    _rewardVector.add(stateJson["Experience Replay"][i]["Reward"].get<float>());

    policy_t expPolicy;
    expPolicy.stateValue = stateJson["Experience Replay"][i]["Experience Policy"]["State Value"].get<float>();
    expPolicy.distributionParameters = stateJson["Experience Replay"][i]["Experience Policy"]["Distribution Parameters"].get<std::vector<float>>();
    expPolicy.actionIndex = stateJson["Experience Replay"][i]["Experience Policy"]["Action Index"].get<size_t>();
    _expPolicyVector.add(expPolicy);

    policy_t curPolicy;
    curPolicy.stateValue = stateJson["Experience Replay"][i]["Current Policy"]["State Value"].get<float>();
    curPolicy.distributionParameters = stateJson["Experience Replay"][i]["Current Policy"]["Distribution Parameters"].get<std::vector<float>>();
    curPolicy.actionIndex = stateJson["Experience Replay"][i]["Current Policy"]["Action Index"].get<size_t>();
  }

  // Restoring training/testing policies
  _trainingCurrentPolicy = stateJson["Training"]["Current Policy"];
  _trainingBestPolicy = stateJson["Training"]["Best Policy"];
  _testingBestPolicy = stateJson["Testing"]["Best Policy"];

  // Setting current agent's training state
  setAgentPolicy(_trainingCurrentPolicy);

  // Resetting the optimizers that the algorithm might be using
  resetAgentOptimizers();

  auto endTime = std::chrono::steady_clock::now();                                                                         // Profiling
  double deserializationTime = std::chrono::duration_cast<std::chrono::nanoseconds>(endTime - beginTime).count() / 1.0e+9; // Profiling
  _k->_logger->logInfo("Detailed", "Took %fs to deserialize training state.\n", deserializationTime);
}

void @className::printGenerationAfter()
{
  if (_mode == "Training")
  {
    _k->_logger->logInfo("Normal", "Replay Experience Statistics:\n");

    _k->_logger->logInfo("Normal", " + Experience Memory Size:      %lu/%lu\n", _stateVector.size(), _experienceReplayMaximumSize);

    if (_maxEpisodes > 0)
      _k->_logger->logInfo("Normal", " + Total Episodes Count:        %lu/%lu\n", _currentEpisode, _maxEpisodes);
    else
      _k->_logger->logInfo("Normal", " + Total Episodes Count:        %lu\n", _currentEpisode);

    if (_maxExperiences > 0)
      _k->_logger->logInfo("Normal", " + Total Experience Count:      %lu/%lu\n", _experienceCount, _maxExperiences);
    else
      _k->_logger->logInfo("Normal", " + Total Experience Count:      %lu\n", _experienceCount);

    if (_rewardOutboundPenalizationEnabled == true)
      _k->_logger->logInfo("Normal", " + Out of Bound Actions:        %lu (%.3f%%)\n", _rewardOutboundPenalizationCount, 100.0f * (float)_rewardOutboundPenalizationEnabled / (float)_experienceCount);

    _k->_logger->logInfo("Normal", "Off-Policy Statistics:\n");
    _k->_logger->logInfo("Normal", " + Count (Ratio/Target):        %lu/%lu (%.3f/%.3f)\n", _experienceReplayOffPolicyCount, _stateVector.size(), _experienceReplayOffPolicyRatio, _experienceReplayOffPolicyTarget);
    _k->_logger->logInfo("Normal", " + Importance Weight Cutoff:    [%.3f, %.3f]\n", 1.0f / _experienceReplayOffPolicyCurrentCutoff, _experienceReplayOffPolicyCurrentCutoff);
    _k->_logger->logInfo("Normal", " + REFER Beta Factor:           %f\n", _experienceReplayOffPolicyREFERBeta);

    _k->_logger->logInfo("Normal", "Training Statistics:\n");

    if (_maxPolicyUpdates > 0)
      _k->_logger->logInfo("Normal", " + Policy Update Count:         %lu/%lu\n", _policyUpdateCount, _maxPolicyUpdates);
    else
      _k->_logger->logInfo("Normal", " + Policy Update Count:         %lu\n", _policyUpdateCount);

    _k->_logger->logInfo("Normal", " + Latest Reward:               %f/%f\n", _trainingLastReward, _problem->_trainingRewardThreshold);
    _k->_logger->logInfo("Normal", " + %lu-Episode Average Reward:  %f\n", _trainingAverageDepth, _trainingAverageReward);
    _k->_logger->logInfo("Normal", " + Best Reward:                 %f (%lu)\n", _trainingBestReward, _trainingBestEpisodeId);

    _k->_logger->logInfo("Normal", " + Rollouts Mean (Sdev) Reward:               %f (%f)\n", _rewardSampleMean, _rewardSampleSdev);
    _k->_logger->logInfo("Normal", " + Rollouts Min/Max Reward:               %f/%f\n", _rewardSampleMin, _rewardSampleMax);
    if (isinf(_problem->_trainingRewardThreshold) == false)
    {
      _k->_logger->logInfo("Normal", "Testing Statistics:\n");

      _k->_logger->logInfo("Normal", " + Candidate Policies:          %lu\n", _testingCandidateCount);

      _k->_logger->logInfo("Normal", " + Latest Average (Stdev / Worst / Best) Reward: %f (%f / %f / %f)\n", _testingAverageReward, _testingStdevReward, _testingWorstReward, _testingBestReward);
    }

    if (_testingTargetAverageReward > -korali::Inf)
      _k->_logger->logInfo("Normal", " + Best Average Reward: %f/%f (%lu)\n", _testingBestAverageReward, _testingTargetAverageReward, _testingBestEpisodeId);
    else
      _k->_logger->logInfo("Normal", " + Best Average Reward: %f (%lu)\n", _testingBestAverageReward, _testingBestEpisodeId);

    printAgentInformation();
    _k->_logger->logInfo("Normal", " + Current Learning Rate:           %.3e\n", _currentLearningRate);

    if (_rewardRescalingEnabled)
      _k->_logger->logInfo("Normal", " + Reward Rescaling:            N(%.3e, %.3e)         \n", _rewardRescalingMean, _rewardRescalingSigma);

    if (_stateRescalingEnabled)
      _k->_logger->logInfo("Detailed", " + Using State Rescaling\n");

    _k->_logger->logInfo("Detailed", "Profiling Information:                  [Generation] - [Session]\n");
    _k->_logger->logInfo("Detailed", " + Experience Serialization Time:       [%5.3fs] - [%3.3fs]\n", _generationSerializationTime / 1.0e+9, _sessionSerializationTime / 1.0e+9);
    _k->_logger->logInfo("Detailed", " + Agent Attending Time:                [%5.3fs] - [%3.3fs]\n", _generationAgentAttendingTime / 1.0e+9, _sessionAgentAttendingTime / 1.0e+9);
    _k->_logger->logInfo("Detailed", " + Avg Agent Computation Time:          [%5.3fs] - [%3.3fs]\n", _generationAgentComputationTime / 1.0e+9, _sessionAgentComputationTime / 1.0e+9);
    _k->_logger->logInfo("Detailed", " + Avg Agent Communication/Wait Time:   [%5.3fs] - [%3.3fs]\n", _generationAgentCommunicationTime / 1.0e+9, _sessionAgentCommunicationTime / 1.0e+9);
    _k->_logger->logInfo("Detailed", " + Avg Agent Policy Evaluation Time:    [%5.3fs] - [%3.3fs]\n", _generationAgentPolicyEvaluationTime / 1.0e+9, _sessionAgentPolicyEvaluationTime / 1.0e+9);
    _k->_logger->logInfo("Detailed", " + Policy Update Time:                  [%5.3fs] - [%3.3fs]\n", _generationPolicyUpdateTime / 1.0e+9, _sessionPolicyUpdateTime / 1.0e+9);
    _k->_logger->logInfo("Detailed", " + Running Time:                        [%5.3fs] - [%3.3fs]\n", _generationRunningTime / 1.0e+9, _sessionRunningTime / 1.0e+9);
    _k->_logger->logInfo("Detailed", " + [I/O] Result File Saving Time:        %5.3fs\n", _k->_resultSavingTime / 1.0e+9);
  }

  if (_mode == "Testing")
  {
    _k->_logger->logInfo("Normal", "Testing Results:\n");
    for (size_t agentId = 0; agentId < _testingSampleIds.size(); agentId++)
    {
      _k->_logger->logInfo("Normal", " + Sample %lu:\n", _testingSampleIds[agentId]);
      _k->_logger->logInfo("Normal", "   + Cumulative Reward               %f\n", _testingReward[agentId]);
    }
  }
}

@moduleAutoCode

@endNamespace
