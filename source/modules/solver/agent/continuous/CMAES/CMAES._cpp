#include "engine.hpp"
#include "modules/solver/agent/continuous/CMAES/CMAES.hpp"
#include "omp.h"
#include "sample/sample.hpp"

@startNamespace

void @className::initializeAgent()
{
  // Initializing common discrete agent configuration
  Continuous::initializeAgent();

  /*********************************************************************
 * Initializing Critic/Policy Neural Network Optimization Experiment
 *********************************************************************/

  _criticPolicyExperiment["Problem"]["Type"] = "Supervised Learning";
  _criticPolicyExperiment["Problem"]["Max Timesteps"] = _timeSequenceLength;
  _criticPolicyExperiment["Problem"]["Training Batch Size"] = _miniBatchSize;
  _criticPolicyExperiment["Problem"]["Inference Batch Size"] = 1;
  _criticPolicyExperiment["Problem"]["Input"]["Size"] = _problem->_stateVectorSize;
  _criticPolicyExperiment["Problem"]["Solution"]["Size"] = _policyParameterCount;

  _criticPolicyExperiment["Solver"]["Type"] = "Learner/DeepSupervisor";
  _criticPolicyExperiment["Solver"]["L2 Regularization"]["Enabled"] = false;
  _criticPolicyExperiment["Solver"]["L2 Regularization"]["Importance"] = 0.0;
  _criticPolicyExperiment["Solver"]["Learning Rate"] = _currentLearningRate;
  _criticPolicyExperiment["Solver"]["Loss Function"] = "Direct Gradient";
  _criticPolicyExperiment["Solver"]["Steps Per Generation"] = 1;
  _criticPolicyExperiment["Solver"]["Neural Network"]["Optimizer"] = _neuralNetworkOptimizer;
  _criticPolicyExperiment["Solver"]["Neural Network"]["Engine"] = _neuralNetworkEngine;
  _criticPolicyExperiment["Solver"]["Neural Network"]["Hidden Layers"] = _neuralNetworkHiddenLayers;
  _criticPolicyExperiment["Solver"]["Output Weights Scaling"] = 0.1;
 
 // Setting transformations for the selected policy distribution output
  for (size_t i = 0; i < _policyParameterCount; i++)
  {
    _criticPolicyExperiment["Solver"]["Neural Network"]["Output Layer"]["Scale"][i] = _policyParameterScaling[i];
    _criticPolicyExperiment["Solver"]["Neural Network"]["Output Layer"]["Shift"][i] = _policyParameterShifting[i];
    _criticPolicyExperiment["Solver"]["Neural Network"]["Output Layer"]["Transformation Mask"][i] = "Identity";
  }

  // Running initialization to verify that the configuration is correct
  _criticPolicyExperiment.initialize();
  _criticPolicyProblem = dynamic_cast<problem::SupervisedLearning *>(_criticPolicyExperiment._problem);
  _criticPolicyLearner = dynamic_cast<solver::learner::DeepSupervisor *>(_criticPolicyExperiment._solver);

  _dampingConstant = 1. + 0.5*_policyParameterCount;
  _successLearningRate = 1./12.;
  _targetSuccessRate = 2./11.;
  _searchPathLearningRate = 2./(_policyParameterCount + 2.);
  _cplus = 2./(_policyParameterCount*_policyParameterCount+6.);
}

void @className::trainPolicy()
{
  size_t hyperparameterCount = _currentPolicyHyperparameter.size();

  // Descale success rate
  _successRate *= (1.-_successLearningRate);   
  if(_rewardSampleMax > _previousRewardSampleMax)
  {
    // Find best idx
    float max = -Inf;
    size_t bestIdx = 0;
    for(size_t m = 0; m < _episodesPerUpdate; ++m)
    {
      if(_rewardSampleVector[m] > max) max = _rewardSampleVector[m]; 
      bestIdx = m;
    }
  
    // Increase success rate
    _successRate += _successLearningRate;

    // Initialize squared l2 norm of w
    float wnorm2 = 0.;

    // Update search path and w
    for(size_t i = 0; i < hyperparameterCount; ++i)
    {
      _searchPath[i] = (1.-_searchPathLearningRate)*_searchPath[i] + std::sqrt(_searchPathLearningRate*(2.-_searchPathLearningRate))*_standardDevs[i]*_sampleVector[bestIdx][i];
      _wVector[i] = _standardDevs[i]/_standardDevs[i]*_sampleVector[bestIdx][i];
      // Accumulate l2 norm
      wnorm2 += _wVector[i]*_wVector[i];
    }

    // Compute coefficients a and b
    float a = std::sqrt(1.-_cplus);
    float b = a/wnorm2*(std::sqrt(1.+_cplus/(1.-_cplus)*wnorm2) - 1.);

    // Update Sdevs
    for(size_t i = 0; i < hyperparameterCount; ++i)
    {
      _standardDevs[i] = a*_standardDevs[i] + b * _standardDevs[i] * _wVector[i] * _wVector[i];
    }

  
  }

  _noiseParameter = _noiseParameter*std::exp(1./_dampingConstant*(_successLearningRate-_targetSuccessRate)/(1.-_targetSuccessRate));

  // Calculate Policy Gradient
  std::vector<float> hyperparameterGradient(hyperparameterCount, 0.0);
  for(size_t m = 0; m < _episodesPerUpdate; m+=2)
  {
    for(size_t i = 0; i < hyperparameterCount; ++i)
      hyperparameterGradient[i] += (_rewardSampleVector[m]-_rewardSampleVector[m+1])*_sampleVector[m][i];
  }
 
  _previousPolicyHyperparameter = _currentPolicyHyperparameter;
  // Scale Gradient and perform update 
  for(size_t i = 0; i < hyperparameterCount; ++i)
  {
    hyperparameterGradient[i] /= (0.5 * _episodesPerUpdate * _rewardSampleSdev);
    _currentPolicyHyperparameter[i] += _learningRate*hyperparameterGradient[i];
  }
    
  _criticPolicyLearner->setHyperparameters(_currentPolicyHyperparameter);
  _policyUpdateCount++;

}

std::vector<policy_t> @className::runPolicy(const std::vector<std::vector<std::vector<float>>> &stateBatch)
{
  // Getting batch size
  size_t batchSize = stateBatch.size();

  // Storage for policy
  std::vector<policy_t> policyVector(batchSize);

  // Forward the neural network for this state
  const auto evaluation = _criticPolicyLearner->getEvaluation(stateBatch);

#pragma omp parallel for
  for (size_t b = 0; b < batchSize; b++)
  {
    // Getting distribution parameters
    policyVector[b].distributionParameters.assign(evaluation[b].begin(), evaluation[b].end());
  }

  return policyVector;
}

knlohmann::json @className::getAgentPolicy()
{
  knlohmann::json hyperparameters;
  hyperparameters["Policy"] = _criticPolicyLearner->getHyperparameters();
  return hyperparameters;
}

void @className::setAgentPolicy(const knlohmann::json &hyperparameters)
{
  _criticPolicyLearner->setHyperparameters(hyperparameters["Policy"].get<std::vector<float>>());
}

void @className::resetAgentOptimizers()
{
  _criticPolicyLearner->resetOptimizer();
}

void @className::printAgentInformation()
{
}

@moduleAutoCode

@endNamespace
