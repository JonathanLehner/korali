*********************************************
SAEM Adjusted for Hierarchical Models
*********************************************

The Stochastic Approximation Expectation Maximization (SAEM) solver is targeted at
a very general form of latent variable problems. If a problem has a hierarchical
form:

.. math::

  p(X, \theta | \psi) = p(X | \theta) \cdot q(\theta | \psi)

Where:
 -  :math:`X = (x_i)_i` are observed data points
    (entirely dealt with by the user, only mentioned for clarity)
 -  :math:`\theta` are latent (i.e. unobserved) variables,
 -  :math:`\psi` are hyperparameters.

:math:`p(X | \theta )` **is given by the user** and can have arbitrary an form
(that is a - possibly unnormalized - probability distribution, i.e. it should have
a finite integral over :math:`X`).

:math:`p(\theta | \psi )` is **generated by Korali**. Each latent variable in :math:`\theta`
can be chosen to have a **normal**, **log-normal** or **logit-normal** distribution.
(Log-normal and logit-normal distributions refer to the log or logit of
:math:`\theta_i` being normally distributed.)


As opposed to SAEM, hSAEM implements sampling for the E-step internally, so no sampler
needs to be given by the user. # Todo: simulated annealing

We exactly follow the method described in section 9.2.4.1 in `Lavielle's book <http://www.cmap.polytechnique.fr/~lavielle/book.html>`_ .



Parameters that influence runtime
---------------------------------
- The number of MCMC steps per generation is (`"Number Samples Per Step"`) *
  (`"N1"` + `"N2"` + `"N3"`) * (`"mcmc Outer Steps"`).
  If hSAEM takes too long, check whether you can reduce this number without impacting accuracy.
- Other parameters influence how many iterations are needed until convergence:

  - If convergence is slow, try reducing `"Alpha 1"`, while keeping
    sufficiently many `"Number Initial Steps"` during which `"Alpha 1"`
    is active.
  - Note: `"Alpha 1"` is only active in genrations `"K1"` to  `"Number Initial Steps"`.
  - Simulated Annealing (SA): Increasing the initial SA variance and
    decreasing its decay factor will increase stochasticity.
  - Note: SA only has an effect if the SA variance is larger than some of the
    diagonal entries of the estimated covariance matrix.


Simulated Annealing
-------------------
Simulated Annealing (SA, see `Kirkpatrick 1984 <https://link.springer.com/article/10.1007/BF01009452>`_ ) is a technique
for stochastic optimization methods, where the probability distribution is widened artificially, by a factor decreasing
with iterations, to allow more exploration and prevent getting stuck in local minima.

Parameters that control SA in the SAEM solver are:

.. code-block::

  


Korali's implementation of SA for SAEM follows the proposal in chapter 9.2.6 in the `book by Lavielle <http://www.cmap.polytechnique.fr/~lavielle/book.html>`_ .



Examples
--------

Four examples how to use this solver can be found in example folder `examples/hierarchical.bayesian/latent.variables/`

- `run-saem-hierarchical.py`
- `run-saem-hierarchical-nd.py`
- `run-saem-normal.py`
- `run-saem-logistic.py`

