#include "modules/conduit/conduit.hpp"
#include "modules/problem/bayesian/latent/hierarchicalLatentCustom/hierarchicalLatentCustom.hpp"
#include "modules/problem/bayesian/latent/hierarchicalLatentReference/hierarchicalLatentReference.hpp"
#include "modules/solver/HSAEM/HSAEM.hpp"

//#include <algorithm>
#include <cmath>
#include <sstream>
#include <vector>

#include <gsl/gsl_math.h>    // todo: Do I need this here? I need a few of them for sure.
#include <gsl/gsl_randist.h> // todo: Do I need this here?
#include <gsl/gsl_rng.h>     // todo: Do I need this here?
#include <gsl/gsl_sf.h>      // todo: Do I need this here?

void korali::solver::HSAEM::runGeneration()
{
  if (_k->_currentGeneration == 1) setInitialConfiguration();

  _k->_logger->logInfo("Normal", "Running generation %lu...\n", _k->_currentGeneration);

  /* E1: Sample latent variable values */
  sampleLatent();
  _k->_logger->logInfo("Detailed", "Sampled generation: %d \n", _k->_currentGeneration);

  /* E2: Update posterior probability function Q */
  updateS();

  /* M:  Find argmax Q(theta), analytically */
  updateHyperparameters();
  // Todo: Might combine them / move them all into updateHyperparameters
  updateDistribution();
  updateProbabilities();
}

/* @brief This is always run before (re-)starting the solver */
void korali::solver::HSAEM::initialize()
{
  if ((_k->_problem->getType() != "Bayesian/Latent/HierarchicalLatentCustom") &&
        (_k->_problem->getType() != "Bayesian/Latent/HierarchicalLatentLowlevel") &&
        (_k->_problem->getType() != "Bayesian/Latent/HierarchicalLatentReference"))
    KORALI_LOG_ERROR("SAEM can only optimize problems of type 'Bayesian/Latent/HierarchicalLatentReference' or '.../HierarchicalLatentCustom' or '.../HierarchicalLatentLowlevel' .\n");
  if (_k->_problem->getType() == "Bayesian/Latent/HierarchicalLatentCustom")
  {
    auto _latentProblemWrapper = dynamic_cast<korali::problem::bayesian::latent::HierarchicalLatentCustom *>(_k->_problem);
    _latentProblem = _latentProblemWrapper->_lowlevelProblem;
  }
  if (_k->_problem->getType() == "Bayesian/Latent/HierarchicalLatentReference")
  {
    auto _latentProblemWrapper = dynamic_cast<korali::problem::bayesian::latent::HierarchicalLatentReference *>(_k->_problem);
    _latentProblem = _latentProblemWrapper->_lowlevelProblem;
  }
  if (_k->_problem->getType() == "Bayesian/Latent/HierarchicalLatentLowlevel")
    _latentProblem = dynamic_cast<korali::problem::bayesian::latent::HierarchicalLatentLowlevel *>(_k->_problem);

  // Todo: make sure the problem is initialized before the solver
  _numberIndividuals = _latentProblem->_numberIndividuals;
  _latentSpaceDimensions = _latentProblem->_latentSpaceDimensions;
  // assert (_latentSpaceDimensions == _k->_variables.size()); // only variables for one individual, as prototypes, should be defined
  // \_ Edit: hSAEM initialization is also called when initializing the sub-problem, from within hierarchicalLatent._cpp. In that case _k has a complete list of variables.
  _mcmcNumberChains = _numberSamplesPerStep; // to simplify things, use one MCMC chain per final sample to generate.

  _univariateNormalGenerator->_mean = 0.0;

  rng = std::mt19937(std::random_device{}());

  for (size_t i = 0; i < _latentSpaceDimensions; i++)
    if (std::isfinite(_k->_variables[i]->_initialValue) == false)
      KORALI_LOG_ERROR("Initial Value of variable \'%s\' not defined (no defaults can be calculated).\n", _k->_variables[i]->_name.c_str());

  if ((_delta < 0.) || (_delta > 1.))
    KORALI_LOG_ERROR("Value of 'delta' must be between 0 and 1. delta was: %.2f\n", _delta);

  if ((_mcmcTargetAcceptanceRate <= 0.) || (_mcmcTargetAcceptanceRate >= 1))
    KORALI_LOG_ERROR("Value of 'mcmc Target Acceptance Rate' must be above 0 and below 1. Value %.2f is invalid.\n", _mcmcTargetAcceptanceRate);
}

/* @brief Run once, before the first generation */
void korali::solver::HSAEM::setInitialConfiguration()
{
  // Resizing and clearing internal vectors
  assert(_mcmcNumberChains == _numberSamplesPerStep); // Else the function above is not called before this one, implementation error, need to switch those
  _currentSamples.resize(_mcmcNumberChains);
  for (size_t i = 0; i < _mcmcNumberChains; i++)
  {
    _currentSamples[i].resize(_numberIndividuals);
    for (size_t j = 0; j < _numberIndividuals; j++)
      _currentSamples[i][j].resize(_latentSpaceDimensions);
  }

  _currentSampleLogLikelihoods.resize(_mcmcNumberChains);
  _currentSampleLogPriors.resize(_mcmcNumberChains);
  for (size_t i = 0; i < _mcmcNumberChains; i++)
  {
    _currentSampleLogLikelihoods[i].resize(_numberIndividuals);
    _currentSampleLogPriors[i].resize(_numberIndividuals);
  }
  _acceptanceRate.resize(_latentSpaceDimensions);
  _acceptanceRateNominator.resize(_latentSpaceDimensions);
  _acceptanceRateDenominator.resize(_latentSpaceDimensions);
  std::fill(_acceptanceRateNominator.begin(), _acceptanceRateNominator.end(), 0.0);
  std::fill(_acceptanceRateDenominator.begin(), _acceptanceRateDenominator.end(), 0.0);
  _currentSampleStandardDeviations.resize(_numberIndividuals);
  _currentSampleMeans.resize(_numberIndividuals);
  for (size_t i = 0; i < _numberIndividuals; i++)
  {
    _currentSampleStandardDeviations[i].resize(_latentSpaceDimensions);
    _currentSampleMeans[i].resize(_latentSpaceDimensions);
  }
  _currentS1.resize(_latentSpaceDimensions);
  _currentS2.resize(_latentSpaceDimensions);
  for (size_t i = 0; i < _latentSpaceDimensions; i++)
    _currentS2[i].resize(_latentSpaceDimensions);
  std::fill(_currentS1.begin(), _currentS1.end(), 0);
  for (size_t i = 0; i < _latentSpaceDimensions; i++)
    std::fill(_currentS2[i].begin(), _currentS2[i].end(), 0);
  //  _currentCholesky.resize(_latentSpaceDimensions);
  //  for (size_t i = 0; i < _latentSpaceDimensions; i++)
  //    _currentCholesky[i].resize(_latentSpaceDimensions);
  _mcmcVariances.resize(_latentSpaceDimensions);
  std::fill(_mcmcVariances.begin(), _mcmcVariances.end(), 1.0);

  _annealedCovariance.resize(_latentSpaceDimensions);
  for (size_t j = 0; j < _latentSpaceDimensions; j++)
  {
    _annealedCovariance[j].resize(_latentSpaceDimensions);
    std::fill(_annealedCovariance[j].begin(), _annealedCovariance[j].end(), 0);
  }
  for (size_t i = 0; i < _latentSpaceDimensions; i++)
    _annealedCovariance[i][i] = _simulatedAnnealingInitialVariance; // "high" initial covariance values; adjust if needed

  // Set starting values for hyperparameters - just copy them over, we already set them in the problem initialization
  _currentHyperparametersMean.resize(_latentSpaceDimensions);
  _currentHyperparametersCovariance.resize(_latentSpaceDimensions);
  for (size_t i = 0; i < _latentSpaceDimensions; i++)
    _currentHyperparametersCovariance[i].resize(_latentSpaceDimensions);
  size_t hyperparam_index = 0;
  for (size_t i = 0; i < _latentSpaceDimensions; i++)
  {
    //    int idx =  _latentProblem->_hyperparametersMeanIndices[i];
    _currentHyperparametersMean[i] = _latentProblem->_hyperparametersMean[i]->_initialValue;
  }
  for (size_t i = 0; i < _latentSpaceDimensions; i++)
    for (size_t j = 0; j < _latentSpaceDimensions; j++)
    {
      //      int idx =  _latentProblem->_hyperparametersCovIndices[i][j];
      int idx = i * _latentProblem->_latentSpaceDimensions + j;
      _currentHyperparametersCovariance[i][j] = _latentProblem->_hyperparametersCovariance[idx]->_initialValue;
    }

  _currentCovarianceCholesky.resize(_latentSpaceDimensions);
  for (size_t i = 0; i < _latentSpaceDimensions; i++)
    _currentCovarianceCholesky[i].resize(_latentSpaceDimensions);
  _currentAnnealedCovarianceCholesky.resize(_latentSpaceDimensions);
  for (size_t i = 0; i < _latentSpaceDimensions; i++)
    _currentAnnealedCovarianceCholesky[i].resize(_latentSpaceDimensions);
  annealCovariance();
  updateCholesky();

  // Initialize the (transformed) latent variables
  updateAnnealedDistribution();
  _currentZ.resize(_mcmcNumberChains);
  for (size_t i = 0; i < _mcmcNumberChains; i++)
  {
    _currentZ[i].resize(_numberIndividuals);
    for (size_t j = 0; j < _numberIndividuals; j++)
    {
      _currentZ[i][j].resize(_latentSpaceDimensions);
      //std::vector<double> temp_something(1);
      _annealedNormalGenerator->getRandomVector(&_currentZ[i][j][0], _latentSpaceDimensions);
    }
  }

  // Initialize log-Likelihoods. Z is initialized above. Do not switch these two paragraphs.
  updateProbabilities(); // Todo: If works, remove what's commented out
                         //  for (size_t c_idx = 0; c_idx < _mcmcNumberChains; c_idx++)
                         //  {
                         //    korali::Sample sample;
                         //    sample["Sample Id"] = 2564234;
                         //    sample["Latent Variables"] = _currentZ[c_idx];
                         //    sample["Operation"] = "Evaluate logLikelihood";
                         //    sample["Module"] = "Problem";
                         //    _conduit->start(sample);
                         //    _conduit->wait(sample);
                         //    auto newLLHs = sample["logLikelihood"].get<std::vector<double>>();
                         //    _currentSampleLogLikelihoods[c_idx] = newLLHs;
                         //  }

  if (_latentSpaceDimensions == 1)
    _n3 = 0;
  if (_latentProblem->_logitnormalLatentIndices.size() > 0.75 * (_latentSpaceDimensions))
    _n1 = 0; // For Logit-Normal variables, the first "chain", first N1 steps, are very unstable. That is
             // because the LLH is a bad indicator for logit-normals, and the first steps rely purely on LLH as acceptance criterion.

  if (_logAllSamples)
  {
    _allSamplesThisIteration.resize(_mcmcNumberChains);
    _allLoglikelihoodsThisIteration.resize(_mcmcNumberChains);
    _allPriorsThisIteration.resize(_mcmcNumberChains);
    for (size_t c = 0; c < _mcmcNumberChains; c++)
    {
      size_t allSteps = _mcmcOuterSteps * (_n1 + _n2 + _n3);
      _allSamplesThisIteration[c].resize(allSteps);
      _allLoglikelihoodsThisIteration[c].resize(allSteps);
      _allPriorsThisIteration[c].resize(allSteps);
      for (size_t st = 0; st < allSteps; st++)
      {
        _allSamplesThisIteration[c][st].resize(_numberIndividuals);
        _allLoglikelihoodsThisIteration[c][st].resize(_numberIndividuals);
        _allPriorsThisIteration[c][st].resize(_numberIndividuals);
        for (size_t j = 0; j < _numberIndividuals; j++)
          _allSamplesThisIteration[c][st][j].resize(_latentSpaceDimensions);
      }
    }
  }

  _bestSampleLogProbability = -korali::Inf;
}

void korali::solver::HSAEM::sampleLatent()
{
  std::fill(_acceptanceRateDenominator.begin(), _acceptanceRateDenominator.end(), 0.0);
  std::fill(_acceptanceRateNominator.begin(), _acceptanceRateNominator.end(), 0.0);

  // only needed if we log all samples
  std::vector<std::vector<std::vector<double>>> allSamples;
  std::vector<std::vector<double>> allLLHs;
  std::vector<std::vector<double>> allPriors;
  size_t counter;

  // ** Loop over chains
  for (size_t c_idx = 0; c_idx < _mcmcNumberChains; c_idx++)
  {
    //std::vector<std::vector<std::vector<double>>> chainSamples; -- these are stored in _currentZ
    //
    std::vector<double> currentLLHs = _currentSampleLogLikelihoods[c_idx];
    std::vector<double> currentPriors(0); // will be used in steps 2 and 3
    std::vector<double> acceptanceRates(0);
    // Access _currentZ only twice for each chain, and hope that this allows parallelism - use chainZ in between:
    std::vector<std::vector<double>> chainZ = _currentZ[c_idx];
    if (_logAllSamples)
    {
      allSamples.resize(_mcmcOuterSteps * (_n1 + _n2 + _n3));
      allLLHs.resize(_mcmcOuterSteps * (_n1 + _n2 + _n3));
      allPriors.resize(_mcmcOuterSteps * (_n1 + _n2 + _n3));
      for (size_t st = 0; st < _mcmcOuterSteps * (_n1 + _n2 + _n3); st++)
      {
        allSamples[st].resize(_numberIndividuals);
        allLLHs[st].resize(_numberIndividuals);
        allPriors[st].resize(_numberIndividuals);
        for (size_t i = 0; i < _numberIndividuals; i++)
          allSamples[st][i].resize(_latentSpaceDimensions);
      }
      counter = 0;
    }

    korali::Sample sample;
    sample["Sample Id"] = 2564236; // Todo: better way to pick an ID?
    sample["Module"] = "Problem";
    sample["Current Generation"] = _k->_currentGeneration;

    // ** For each chain, the zeta values to restart from are: _currentZ

    // ** Loop over one chain
    for (size_t step = 0; step < _mcmcOuterSteps; step++)
    {
      // *****************************
      // ** 1st proposal distribution:
      // *****************************
      //    Sample directly according to p(latent | hyperparameter), ignoring the data. Accept according to the ratios
      //       of the data likelihoods.
      for (size_t i = 0; i < _n1; i++)
      {
        // Proposals q1: Drawn from p(latent | hyperparam)
        std::vector<std::vector<double>> newZ(_numberIndividuals); // NrIndividuals x NrDimensions
        std::vector<double> tempZ(_latentSpaceDimensions);
        for (size_t indiv = 0; indiv < _numberIndividuals; indiv++)
        {
          _annealedNormalGenerator->getRandomVector(&tempZ[0], _latentSpaceDimensions); // == Previous samples minus par.beta --Why?
          newZ[indiv] = tempZ;                                                          // TODO: Check that this is not copy-by-reference or something
        }

        // generate acceptance probabilities: new llh / former llh
        sample["Latent Variables"] = newZ; // Note: We need not transform to non-Z form; the function called with "Evaluate logLikelihood" expects Z form
        sample["Operation"] = "Evaluate logLikelihood";
        _conduit->start(sample);
        _conduit->wait(sample);
        auto newLLHs = sample["logLikelihood"].get<std::vector<double>>();
        for (size_t indiv = 0; indiv < _numberIndividuals; indiv++)
        {
          // generate RVs to randomly accept or reject each new zeta
          double logAcceptanceProb = newLLHs[indiv] - currentLLHs[indiv];
          double rval = _uniformGenerator->getRandomNumber();
          if (log(rval) < logAcceptanceProb)
          {
            // update zeta, update likelihood
            chainZ[indiv] = newZ[indiv];
            // Todo: Do not write to currentZ directly here, instead create a temporary array and write back everything
            //             only after the loop over all chains? I don't know enough about the optimization / what will be
            //             parallelized in both cases. It could help.
            // Todo: Look at (T)MCMC, how are separate chains managed there?
            currentLLHs[indiv] = newLLHs[indiv];
          }
        }
        if (_logAllSamples)
        {
          allSamples[counter] = chainZ;
          allLLHs[counter] = currentLLHs;
          allPriors[counter] = std::vector<double>(_numberIndividuals, 0); // we don't track priors here
          counter++;
        }
      }

      // initialize log priors
      sample["Latent Variables"] = chainZ;
      sample["Mean"] = _currentHyperparametersMean;
      sample["Covariance Cholesky Decomposition"] = _currentAnnealedCovarianceCholesky;
      sample["Operation"] = "Evaluate logPrior";
      _conduit->start(sample);
      _conduit->wait(sample);
      currentPriors = sample["Log Prior"].get<std::vector<double>>();

      // *****************************
      // ** 2nd proposal distribution
      // *****************************
      //    sample each coordinate separately, according to a normal distribution unrelated to any variable
      //    --> the acceptance probability is then a function of latent variables and hyperparameters
      for (size_t i = 0; i < _n2; i++)
      {
        for (size_t dim = 0; dim < _latentSpaceDimensions; dim++)
        {
          // we will use this to sample, and then update _mcmcVariances after step 2, and then again after step 3
          _univariateNormalGenerator->_standardDeviation = sqrt(_mcmcVariances[dim]);
          _univariateNormalGenerator->updateDistribution();
          //          assert(_univariateNormalGenerator->_standardDeviation != 1.);

          std::vector<std::vector<double>> localZ = chainZ;
          for (size_t indiv = 0; indiv < _numberIndividuals; indiv++)
          {
            double eta = _univariateNormalGenerator->getRandomNumber(); // Todo: Adjust covariance below - check whether we count acceptance for each coordinate or how
            // Adjust eta, eta is ~ N(0, 1) and we need other sdev
            //            eta *= sqrt(_mcmcVariances[dim]);
            localZ[indiv][dim] += eta;
          }

          sample["Latent Variables"] = localZ;
          sample["Operation"] = "Evaluate logLikelihood";
          _conduit->start(sample);
          _conduit->wait(sample);
          auto newLLHs = sample["logLikelihood"].get<std::vector<double>>();

          sample["Latent Variables"] = localZ;
          sample["Mean"] = _currentHyperparametersMean;
          sample["Covariance Cholesky Decomposition"] = _currentAnnealedCovarianceCholesky; // _currentHyperparametersCovariance;
          sample["Operation"] = "Evaluate logPrior";
          _conduit->start(sample);
          _conduit->wait(sample);
          auto newPriors = sample["Log Prior"].get<std::vector<double>>();

          // acceptance probability == ratio of new prior * log to the current one
          for (size_t indiv = 0; indiv < _numberIndividuals; indiv++)
          {
            double logAcceptanceProb = newLLHs[indiv] + newPriors[indiv] - currentLLHs[indiv] - currentPriors[indiv];
            double rval = _uniformGenerator->getRandomNumber();
            if (log(rval) < logAcceptanceProb)
            {
              chainZ[indiv] = localZ[indiv];
              _acceptanceRateNominator[dim] += 1.0;
              currentLLHs[indiv] = newLLHs[indiv];
              currentPriors[indiv] = newPriors[indiv];
            }
          }
          _acceptanceRateDenominator[dim] += float(_numberIndividuals);
        }
        if (_logAllSamples)
        {
          allSamples[counter] = chainZ;
          allLLHs[counter] = currentLLHs;
          allPriors[counter] = currentPriors;
          counter++;
        }
      }

      // **  update variances
      if (_n2 > 0)
      {
        for (size_t dim = 0; dim < _latentSpaceDimensions; dim++)
        {
          _acceptanceRate[dim] = (_acceptanceRateNominator[dim]) / (_acceptanceRateDenominator[dim]);
          _mcmcVariances[dim] *= (1.0 + _delta * (_acceptanceRate[dim] - _mcmcTargetAcceptanceRate));
          assert(_mcmcVariances[dim] > 0.);
        }
      }

      // *****************************
      // ** 3rd proposal distribution:
      //    - Sample random subsets of coordinates jointly
      // *****************************
      for (size_t i = 0; i < _n3; i++)
      {
        // pick a number of coordinates to change simultaneously
        double nRandCoords_d = _uniformGenerator->getRandomNumber() * double(_latentSpaceDimensions) + 1.;
        if (size_t(nRandCoords_d) == _latentSpaceDimensions + 1) // Todo: Check the cast works
          nRandCoords_d = 1.;
        size_t nRandCoords = size_t(nRandCoords_d); // Todo: Check the cast works

        std::vector<size_t> randomCoords(_latentSpaceDimensions);
        std::iota(randomCoords.begin(), randomCoords.end(), 0);
        std::shuffle(randomCoords.begin(), randomCoords.end(), rng);
        assert(nRandCoords <= randomCoords.size());
        randomCoords.resize(nRandCoords);

        std::vector<std::vector<double>> localZ = chainZ;
        for (size_t dim : randomCoords)
        {
          _univariateNormalGenerator->_standardDeviation = sqrt(_mcmcVariances[dim]);
          _univariateNormalGenerator->updateDistribution();
          //          assert(_univariateNormalGenerator->_standardDeviation != 1.);

          for (size_t indiv = 0; indiv < _numberIndividuals; indiv++)
          {
            double eta = _univariateNormalGenerator->getRandomNumber();
            // Adjust eta, eta is ~ N(0, 1) and we need other sdev
            //            eta *= _mcmcVariances[dim];
            localZ[indiv][dim] += eta;
          }
        }
        // Evaluate localZ proposals for each individual
        sample["Latent Variables"] = localZ;
        sample["Operation"] = "Evaluate logLikelihood";
        _conduit->start(sample);
        _conduit->wait(sample);
        auto newLLHs = sample["logLikelihood"].get<std::vector<double>>();

        sample["Latent Variables"] = localZ;
        sample["Mean"] = _currentHyperparametersMean;
        sample["Covariance Cholesky Decomposition"] = _currentAnnealedCovarianceCholesky;
        sample["Operation"] = "Evaluate logPrior";
        _conduit->start(sample);
        _conduit->wait(sample);
        auto newPriors = sample["Log Prior"].get<std::vector<double>>();

        // Accept? -- Same acceptance calculation as in 2nd proposal distribution.
        for (size_t indiv = 0; indiv < _numberIndividuals; indiv++)
        {
          double logAcceptanceProb = newLLHs[indiv] + newPriors[indiv] - currentLLHs[indiv] - currentPriors[indiv];
          double rval = _uniformGenerator->getRandomNumber();
          if (log(rval) < logAcceptanceProb)
          {
            chainZ[indiv] = localZ[indiv];
            currentLLHs[indiv] = newLLHs[indiv];
            currentPriors[indiv] = newPriors[indiv];
            for (size_t dim : randomCoords)
            {
              _acceptanceRateNominator[dim] += 1.0;
            }
          }
          for (size_t dim : randomCoords)
            _acceptanceRateDenominator[dim] += 1.0;
        }
        if (_logAllSamples)
        {
          allSamples[counter] = chainZ;
          allLLHs[counter] = currentLLHs;
          allPriors[counter] = currentPriors;
          counter++;
        }
      }
      // **  update variances
      if (_n3 > 0)
        for (size_t dim = 0; dim < _latentSpaceDimensions; dim++)
        {
          if (_acceptanceRateDenominator[dim] == 0)
            continue; // This can happen if a coordinate is never selected as part of any random set
          _acceptanceRate[dim] = (_acceptanceRateNominator[dim]) / (_acceptanceRateDenominator[dim]);
          _mcmcVariances[dim] *= (1.0 + _delta * (_acceptanceRate[dim] - _mcmcTargetAcceptanceRate));
          assert(_mcmcVariances[dim] > 0.);
        }

    } // Chain loop end

    _currentSampleLogLikelihoods[c_idx] = currentLLHs;
    _currentSampleLogPriors[c_idx] = currentPriors;

    _currentZ[c_idx] = chainZ;
    for (size_t indiv = 0; indiv < _numberIndividuals; indiv++)
    {
      auto dbg_var = chainZ[indiv];
      _currentSamples[c_idx][indiv] = _latentProblem->zToLatent(chainZ[indiv]);
    }

    if (_logAllSamples)
    {
      _allPriorsThisIteration[c_idx] = allPriors;
      _allLoglikelihoodsThisIteration[c_idx] = allLLHs;
      _allSamplesThisIteration[c_idx] = allSamples;
    }

  } // Loop over all chains end

  // mean and variance:
  auto transposed3D = transpose3D(_currentSamples);
  for (size_t indiv = 0; indiv < _numberIndividuals; indiv++)
  {
    auto transposed2D = transposed3D[indiv];
    assert(transposed2D.size() == _latentSpaceDimensions);
    _currentSampleMeans[indiv] = std::vector<double>(transposed2D.size(), 0.0);
    _currentSampleStandardDeviations[indiv] = std::vector<double>(transposed2D.size(), 0.0);

    for (size_t i = 0; i < transposed2D.size(); i++)
    {
      std::vector<double> mean_and_sdev = meanAndSDev(transposed2D[i]);
      _currentSampleMeans[indiv][i] = mean_and_sdev[0];
      _currentSampleStandardDeviations[indiv][i] = mean_and_sdev[1];
    }
  }
}

//* @brief Anneal the covariance if needed.
void korali::solver::HSAEM::annealCovariance()
{
  if (_useSimulatedAnnealing & (_ka >= _k->_currentGeneration))
  { /* If I'm not mistaken, taking the max on the diagonal can never be a problem because it's the same as
       adding a positive (>=0) diagonal matrix to the covariance, which should preserve positive-definiteness.
       On the other hand, (by proof of 'it happened'), max on off-diagonals (that is, max(cov[i][j], 0) with i !=0)
       can cause negative eigenvalues and can make the covariance invalid. */
    for (size_t i = 0; i < _latentSpaceDimensions; i++)
      for (size_t j = 0; j < _latentSpaceDimensions; j++)
      {
        if (i != j)
          _annealedCovariance[i][j] = _currentHyperparametersCovariance[i][j];
        else
          _annealedCovariance[i][i] = std::max(_annealedCovariance[i][i] * _simulatedAnnealingDecayFactor,
                                               _currentHyperparametersCovariance[i][i]);
      }
  }
  else
    _annealedCovariance = _currentHyperparametersCovariance;
}

//* @brief Update internal distribution representation of p(latent | hyper). ! Does not anneal the covariance.
void korali::solver::HSAEM::updateAnnealedDistribution()
{
  _annealedNormalGenerator->setProperty("Sigma", flatten(_currentAnnealedCovarianceCholesky));
  _annealedNormalGenerator->setProperty("Mean Vector", _currentHyperparametersMean);
  _annealedNormalGenerator->updateDistribution();
  //  if (_useSimulatedAnnealing)
  //  {
  //    auto flatCov = flatten(_currentHyperparametersCovariance);
  //    for (size_t i = 0; i < flatCov.size(); i++)
  //      _annealedCovariance[i] = std::max(_annealedCovariance[i] * _simulatedAnnealingDecayFactor, flatCov[i]);
  //    _annealedNormalGenerator->setProperty("Sigma", _currentAnnealedCovarianceCholesky);
  //  }
  //  else
  //    _annealedNormalGenerator->setProperty("Sigma", flatten(_currentHyperparametersCovarianceCholesky));
  //  _annealedNormalGenerator->setProperty("Mean Vector", _currentHyperparametersMean);
  //  _annealedNormalGenerator->updateDistribution();
}

void korali::solver::HSAEM::updateS()
{
  // Determine alpha
  double alpha;
  if (_k->_currentGeneration > _numberInitialSteps)
    alpha = _alpha2;
  else
    alpha = _alpha1;

  // --> decay factor gamma
  double curGen = double(_k->_currentGeneration);
  if (curGen <= _k1)
    _gamma = 1.0;
  else
    _gamma = std::pow((curGen - _k1), -alpha);

  // * update S1
  for (size_t dim = 0; dim < _latentSpaceDimensions; dim++)
  {
    double sumZ = 0;
    for (size_t chain = 0; chain < _mcmcNumberChains; chain++)
      for (size_t indiv = 0; indiv < _numberIndividuals; indiv++)
        sumZ += _currentZ[chain][indiv][dim];
    _currentS1[dim] = _currentS1[dim] + _gamma * (sumZ / double(_mcmcNumberChains) - _currentS1[dim]);
  }
  // * update S2
  for (size_t i = 0; i < _latentSpaceDimensions; i++)
  {
    for (size_t j = 0; j < _latentSpaceDimensions; j++)
    {
      double sumZiZj = 0;
      for (size_t chain = 0; chain < _mcmcNumberChains; chain++)
        for (size_t indiv = 0; indiv < _numberIndividuals; indiv++)
          sumZiZj += _currentZ[chain][indiv][i] * _currentZ[chain][indiv][j];
      _currentS2[i][j] = _currentS2[i][j] + _gamma * (sumZiZj / double(_mcmcNumberChains) - _currentS2[i][j]);
    }
  }
}

void korali::solver::HSAEM::updateHyperparameters()
{
  for (size_t i = 0; i < _latentSpaceDimensions; i++)
    _currentHyperparametersMean[i] = _currentS1[i] / double(_numberIndividuals);
  for (size_t i = 0; i < _latentSpaceDimensions; i++)
    for (size_t j = 0; j < _latentSpaceDimensions; j++)
    {
      _currentHyperparametersCovariance[i][j] = _currentS2[i][j] / double(_numberIndividuals) - _currentHyperparametersMean[i] * _currentHyperparametersMean[j];
      if (_diagonalCovariance)
        if (i != j)
          _currentHyperparametersCovariance[i][j] = 0;
    }
  // No simulated annealing here, that is only done with the 'annealed normal distribution' used in sampling.
  _currentHyperparametersMeanTransformed = _latentProblem->zToLatent(_currentHyperparametersMean);
}

/* Call after updating hyperparameters: Re-calculate the likelihood of all current samples.
   Need to be already set up: _currentHyperparametersMean and _currentHyperparametersCovariance and _currentZ
*/
void korali::solver::HSAEM::updateProbabilities()
{
  for (size_t c_idx = 0; c_idx < _mcmcNumberChains; c_idx++)
  {
    korali::Sample sample;
    sample["Sample Id"] = 2564242;
    sample["Current Generation"] = _k->_currentGeneration;
    sample["Latent Variables"] = _currentZ[c_idx];
    sample["Operation"] = "Evaluate logLikelihood";
    sample["Module"] = "Problem";
    _conduit->start(sample);
    _conduit->wait(sample);
    auto newLLHs = sample["logLikelihood"].get<std::vector<double>>();
    _currentSampleLogLikelihoods[c_idx] = newLLHs;

    sample["Operation"] = "Evaluate logPrior";
    sample["Latent Variables"] = _currentZ[c_idx];
    sample["Mean"] = _currentHyperparametersMean;
    sample["Covariance Cholesky Decomposition"] = _currentAnnealedCovarianceCholesky;
    _conduit->start(sample);
    _conduit->wait(sample);
    auto newPriors = sample["Log Prior"].get<std::vector<double>>();
    _currentSampleLogPriors[c_idx] = newPriors;
  }
  //  double LLHSum = 0;
  //  for (size_t c_idx = 0; c_idx <_mcmcNumberChains; c_idx++){
  //    LLHSum = std::accumulate(_currentSampleLogLikelihoods[c_idx].begin(), _currentSampleLogLikelihoods[c_idx].end(), LLHSum);
  //    LLHSum = std::accumulate(_currentSampleLogPriors[c_idx].begin(), _currentSampleLogPriors[c_idx].end(), LLHSum);
  //  }
  //  _currentLogLikelihood = LLHSum / double(_mcmcNumberChains);
  _currentSampleLogProbability = 0;
  _currentSampleLogPrior = 0;
  _currentSampleLogLikelihood = 0;
  for (size_t c_idx = 0; c_idx < _mcmcNumberChains; c_idx++)
  {
    _currentSampleLogLikelihood = std::accumulate(_currentSampleLogLikelihoods[c_idx].begin(),
                                                  _currentSampleLogLikelihoods[c_idx].end(),
                                                  _currentSampleLogLikelihood);
    _currentSampleLogProbability = std::accumulate(_currentSampleLogLikelihoods[c_idx].begin(),
                                                   _currentSampleLogLikelihoods[c_idx].end(),
                                                   _currentSampleLogProbability);
    _currentSampleLogProbability = std::accumulate(_currentSampleLogPriors[c_idx].begin(),
                                                   _currentSampleLogPriors[c_idx].end(),
                                                   _currentSampleLogProbability);
    _currentSampleLogPrior = std::accumulate(_currentSampleLogPriors[c_idx].begin(),
                                             _currentSampleLogPriors[c_idx].end(),
                                             _currentSampleLogPrior);
  }
  _currentSampleLogProbability /= double(_mcmcNumberChains);
  _currentSampleLogPrior /= double(_mcmcNumberChains);
  _currentSampleLogLikelihood /= double(_mcmcNumberChains);

  if (_currentSampleLogProbability > _bestSampleLogProbability)
    _bestSampleLogProbability = _currentSampleLogProbability;
}

/* Re-calculate the cholesky decomposition of the annealed covariance matrix.
   Also re-calculate a cholesky decomposition of the true current esimate of the covariance. */
void korali::solver::HSAEM::updateCholesky()
{
  size_t N = _latentSpaceDimensions; // for brevity

  gsl_matrix *covMatrixGSL = gsl_matrix_alloc(N, N);
  for (size_t i = 0; i < N; i++)
    for (size_t j = 0; j < N; j++)
      gsl_matrix_set(covMatrixGSL, i, j, _annealedCovariance[i][j]);

  // First, calculate the cholesky decomposition, cov = L'*L
  gsl_matrix *chol = gsl_matrix_alloc(N, N);
  gsl_matrix_memcpy(chol, covMatrixGSL);

  int status = gsl_linalg_cholesky_decomp1(chol); //lower triangular part + diagonal of chol afterwards contain L from the cholesky decomposition cov = L*L'
  if (status != 0)
  {
    if (status != 1) KORALI_LOG_ERROR("Status == 1 would have indicated an invalid input, such as not positive definite (e.g. very small eigenvalues). But the GSL error code was: %d. Please report this if you think it is a bug.", status, status);
    /* If status == 1, covMatrixGSL was not positive definite.
     * Let's add a small diagonal matrix. */
    double eps = 1.e-4; // 1.e-8 is too small. Maybe GSL has low precision for matrix operations?
    for (size_t i = 0; i < N; i++)
    {
      for (size_t j = 0; j < N; j++)
        gsl_matrix_set(chol, i, j, _annealedCovariance[i][j]);
      gsl_matrix_set(chol, i, i, _annealedCovariance[i][i] + eps);
    }
    int status = gsl_linalg_cholesky_decomp1(chol); //lower triangular part + diagonal of chol afterwards contain L from the cholesky decomposition cov = L*L'

    if (status != 0)
    { // Now it failed for real. Is it negative definite?
      std::ostringstream covLogStringStream;
      _k->_logger->logInfo("Minimal", "Error; printing the annealed covariance matrix: \n");
      _k->_logger->logInfo("Normal", "  [\n");
      for (size_t i = 0; i < _latentSpaceDimensions; i++)
      {
        covLogStringStream << "    [";
        for (size_t j = 0; j < _latentSpaceDimensions; j++)
          covLogStringStream << "\t" << _annealedCovariance[i][j] << " ";
        covLogStringStream << "],\n";
        _k->_logger->logInfo("Normal", covLogStringStream.str().c_str());
        covLogStringStream.str("");
      }
      _k->_logger->logInfo("Normal", "  ]\n");
      _k->_logger->logInfo("Minimal", "Now printing the non-annealed covariance matrix: \n");
      _k->_logger->logInfo("Normal", "  [\n");
      for (size_t i = 0; i < _latentSpaceDimensions; i++)
      {
        covLogStringStream << "    [";
        for (size_t j = 0; j < _latentSpaceDimensions; j++)
          covLogStringStream << "\t" << _currentHyperparametersCovariance[i][j] << " ";
        covLogStringStream << "],\n";
        _k->_logger->logInfo("Normal", covLogStringStream.str().c_str());
        covLogStringStream.str("");
      }
      _k->_logger->logInfo("Normal", "  ]\n");
      KORALI_LOG_ERROR("GSL error code: %d. If ==1: Covariance matrix was invalid; this could mean it had a negative eigenvalue (that should be impossible) or one of its entries was NaN. Please report this if you think it is a bug.", status);
    }
  }
  // Set the cholesky matrix
  for (size_t i = 0; i < N; i++)
    for (size_t j = 0; j < N; j++)
      _currentAnnealedCovarianceCholesky[i][j] = gsl_matrix_get(chol, i, j);

  /* Pt. 2: If we use SA, annealedCov != Cov, so keep track of the original covariance's cholesky decomp additionally.
  */
  if (_useSimulatedAnnealing)
  {
    for (size_t i = 0; i < N; i++)
      for (size_t j = 0; j < N; j++)
        gsl_matrix_set(covMatrixGSL, i, j, _currentHyperparametersCovariance[i][j]);
    gsl_matrix_memcpy(chol, covMatrixGSL);

    int status = gsl_linalg_cholesky_decomp1(chol); //lower triangular part + diagonal of chol afterwards contain L from the cholesky decomposition cov = L*L'
    if (status != 0)
    {
      if (status != 1) KORALI_LOG_ERROR("Status == 1 would have indicated an invalid input, such as not positive definite (e.g. very small eigenvalues). But the GSL error code was: %d. Please report this if you think it is a bug.", status, status);
      /* If status == 1, covMatrixGSL was not positive definite.
       * Let's add a small diagonal matrix. */
      double eps = 1.e-4; // 1.e-8 is too small. Maybe GSL has low precision for matrix operations?
      for (size_t i = 0; i < N; i++)
      {
        for (size_t j = 0; j < N; j++)
          gsl_matrix_set(chol, i, j, _currentHyperparametersCovariance[i][j]);
        gsl_matrix_set(chol, i, i, _currentHyperparametersCovariance[i][i] + eps);
      }
      int status = gsl_linalg_cholesky_decomp1(chol); //lower triangular part + diagonal of chol afterwards contain L from the cholesky decomposition cov = L*L'

      if (status != 0) // Now it failed for real. Is it negative definite?
        KORALI_LOG_ERROR("GSL error code: %d. If ==1: (Non-annealed) covariance matrix was invalid; this could mean it had a negative eigenvalue (that should be impossible) or one of its entries was NaN. Please report this if you think it is a bug.", status);
    }
    // Set the cholesky matrix
    for (size_t i = 0; i < N; i++)
      for (size_t j = 0; j < N; j++)
        _currentCovarianceCholesky[i][j] = gsl_matrix_get(chol, i, j);
  }

  gsl_matrix_free(covMatrixGSL);
  gsl_matrix_free(chol);
}

/** A function to combine the sub-steps in the right order:
    - new covariance --> annealed covariance
        (also needed without SA because we use _annealedCovariance everywhere (it's just the same if no SA))
    - update chol of annealed covariance;
      update chol of original covariance
    - reset sampling distribution to new annealed covariance
    */
void korali::solver::HSAEM::updateDistribution()
{
  annealCovariance();
  updateCholesky();
  updateAnnealedDistribution(); // because the covariance changed
}

/** @brief Utility function to calculate mean and standard deviation of the values in vector v. */
std::vector<double> korali::solver::HSAEM::meanAndSDev(std::vector<double> v)
{
  // Todo: To save space and not duplicate code, use gsl_stats_mean() and gsl_stats_sd_m() instead.

  // Origin: https://stackoverflow.com/questions/7616511/calculate-mean-and-standard-deviation-from-a-vector-of-samples-in-c-using-boos
  double sum = std::accumulate(v.begin(), v.end(), 0.0);
  double mean = sum / double(v.size());

  std::vector<double> diff(v.size());
  std::transform(v.begin(), v.end(), diff.begin(), [mean](double x) { return x - mean; });
  double sq_sum = std::inner_product(diff.begin(), diff.end(), diff.begin(), 0.0);
  double stdev = std::sqrt(sq_sum / double(v.size()));
  std::vector<double> result = {mean, stdev};
  return result;
}

/** @brief Utility function, "transposes" a vector of vectors of vectors @param data
        so that data[i][j][k] will be moved to result[j][k][i].
    */
std::vector<std::vector<std::vector<double>>> korali::solver::HSAEM::transpose3D(const std::vector<std::vector<std::vector<double>>> data)
{
  /* From: https://stackoverflow.com/questions/6009782/how-to-pivot-a-vector-of-vectors */
  // this assumes that all inner vectors have the same size
  std::vector<std::vector<std::vector<double>>> result(data[0].size(), std::vector<std::vector<double>>(data[0][0].size(), std::vector<double>(data.size())));
  for (std::vector<double>::size_type j = 0; j < data[0].size(); j++)
    for (std::vector<double>::size_type k = 0; k < data[0][0].size(); k++)
      for (std::vector<double>::size_type i = 0; i < data.size(); i++)
        result[j][k][i] = data[i][j][k];
  return result;
}

void korali::solver::HSAEM::printGenerationBefore()
{
  _k->_logger->logInfo("Normal", "Preparing to start generation...\n");
}

void korali::solver::HSAEM::printGenerationAfter()
{
  _k->_logger->logInfo("Normal", "Finished generation %lu...\n", _k->_currentGeneration);

  _k->_logger->logInfo("Normal", "    Current Total Log Probability:       %.2e\n", _currentSampleLogProbability);
  _k->_logger->logInfo("Normal", "    Best Total Log Probability:          %.2e\n", _bestSampleLogProbability);
  // ** Show current hyperparameters
  _k->_logger->logInfo("Normal", "\n");
  _k->_logger->logInfo("Normal", " Current hyperparameters, mean: \n");
  for (size_t dim = 0; dim < _latentSpaceDimensions; dim++)
  {
    size_t idx = _latentProblem->_firstIndividualLatentIndices[dim];
    std::string varName = _latentProblem->_k->_variables[idx]->_name;
    _k->_logger->logInfo("Normal", "\tMean for %s etc: \t%.2f\tBack-transf.: %.2f\n", varName.c_str(), _currentHyperparametersMean[dim], _currentHyperparametersMeanTransformed[dim]);
    // _k->_logger->logInfo("Normal", "\t\t%.2f  \n",_currentHyperparametersMean[dim]);
  }
  std::ostringstream covLogStringStream;
  _k->_logger->logInfo("Normal", "Current hyperparameters, covariance matrix: \n");
  //covLogStringStream << "  [\n";
  _k->_logger->logInfo("Normal", "  [\n");
  for (size_t i = 0; i < _latentSpaceDimensions; i++)
  {
    covLogStringStream << "    [";
    for (size_t j = 0; j < _latentSpaceDimensions; j++)
      covLogStringStream << "\t" << _currentHyperparametersCovariance[i][j] << " ";
    covLogStringStream << "],\n";
    _k->_logger->logInfo("Normal", covLogStringStream.str().c_str());
    covLogStringStream.str("");
  }
  _k->_logger->logInfo("Normal", "  ]\n");
  // ** Show:
  //    "Current Sample Means"
  //    "Current Sample Standard Deviations"
  //    "mcmc Standard Deviations"
  _k->_logger->logInfo("Detailed", "    - Current latent variable sample means:\n"); //  of chain %d: \n", c_idx);
  for (size_t i = 0; i < _numberIndividuals; i++)
  {
    _k->_logger->logInfo("Detailed", "      - Individual %d: \n", i);
    for (size_t dim = 0; dim < _latentSpaceDimensions; dim++)
    {
      int idx = _latentProblem->_latentIndex[i][dim];
      _k->_logger->logInfo("Detailed", "        %s : \t%.2f +- %.2f \n", _latentProblem->_k->_variables[idx]->_name.c_str(), _currentSampleMeans[i][dim], _currentSampleStandardDeviations[i][dim]);
    }
  }
  //      for (size_t i = 0; i < _numberIndividuals; i++)
  //      { // ** In modus "detailed", also print llhs and acceptance
  //        _k->_logger->logInfo("Detailed", "      - Individual %d: \n", i);
  //        _k->_logger->logInfo("Detailed", "          LLH:       %.2f \n", _currentSampleLogLikelihoods[c_idx][i]);
  //        _k->_logger->logInfo("Detailed", "          Log-Prior: %.2f: \n", _currentSampleLogPriors[c_idx][i]);
  //      }
  _k->_logger->logInfo("Detailed", "\n");
  _k->_logger->logInfo("Detailed", "    - Acceptance rates: (target: %.2f)\n", _mcmcTargetAcceptanceRate);
  for (size_t dim = 0; dim < _latentSpaceDimensions; dim++)
    _k->_logger->logInfo("Detailed", "         Dim %d: %.3f \n", dim, _acceptanceRate[dim]);

  _k->_logger->logInfo("Detailed", "    - MCMC standard deviations after chain %d:\n", _mcmcNumberChains);
  for (size_t dim = 0; dim < _latentSpaceDimensions; dim++)
    _k->_logger->logInfo("Detailed", "         Dim %d: %.3f \n", dim, sqrt(_mcmcVariances[dim]));

  //* Todo?: Show current mean log-likelihood across samples
}

void korali::solver::HSAEM::finalize()
{
  _k->_logger->logInfo("Normal", "\n");
  _k->_logger->logInfo("Normal", "Finished. Final hyperparameter estimates:\n");
  // Only enable as soon as I have good estimates of that:
  //  _k->_logger->logInfo("Normal", "  Current Total Log Probability:       %.2e\n", ...);
  //  _k->_logger->logInfo("Normal", "  Best Total Log Probability:          %.2e\n", ...);

  // * Set results and print them
  (*_k)["Results"]["Mean"] = _currentHyperparametersMean;
  (*_k)["Results"]["Mean transformed back"] = _currentHyperparametersMeanTransformed;
  (*_k)["Results"]["Covariance Matrix"] = _currentHyperparametersCovariance;

  for (size_t dim = 0; dim < _latentSpaceDimensions; dim++)
  {
    size_t idx = _latentProblem->_firstIndividualLatentIndices[dim];
    std::string varName = _latentProblem->_k->_variables[idx]->_name;
    _k->_logger->logInfo("Normal", "\tMean for %s etc: \t%.2f\tBack-transf.: %.2f\n", varName.c_str(), _currentHyperparametersMean[dim], _currentHyperparametersMeanTransformed[dim]);
  }

  std::ostringstream covLogStringStream;
  _k->_logger->logInfo("Normal", "\n");
  _k->_logger->logInfo("Normal", "Covariance matrix: \n");
  _k->_logger->logInfo("Normal", "  [\n");
  for (size_t i = 0; i < _latentSpaceDimensions; i++)
  {
    covLogStringStream << "    [";
    for (size_t j = 0; j < _latentSpaceDimensions; j++)
      covLogStringStream << "\t" << _currentHyperparametersCovariance[i][j] << " ";
    covLogStringStream << "],\n";
    _k->_logger->logInfo("Normal", covLogStringStream.str().c_str());
    covLogStringStream.str("");
  }
  _k->_logger->logInfo("Normal", "  ]\n");
}

/** @brief: Utility function to convert a vector of vectors into a concatenated 1D vector.
  */
std::vector<double> korali::solver::HSAEM::flatten(const std::vector<std::vector<double>> &v)
{
  // credits to Sebastian Redl @ stackoverflow
  std::size_t total_size = 0;
  for (const auto &sub : v)
    total_size += sub.size();
  std::vector<double> result;
  result.reserve(total_size);
  for (const auto &sub : v)
    result.insert(result.end(), sub.begin(), sub.end());
  return result;
}
