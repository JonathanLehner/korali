#include "modules/conduit/conduit.hpp"
#include "modules/solver/HSAEM/HSAEM.hpp"

#include <algorithm>
#include <vector>
#include <sstream>

#include <gsl/gsl_math.h>       // todo: Do I need this here? I need a few of them for sure.
#include <gsl/gsl_randist.h>    // todo: Do I need this here?
#include <gsl/gsl_rng.h>        // todo: Do I need this here?
#include <gsl/gsl_sf.h>         // todo: Do I need this here?


void korali::solver::HSAEM::runGeneration()
{
  if (_k->_currentGeneration == 1) setInitialConfiguration();

  _k->_logger->logInfo("Normal", "Running generation %lu...\n", _k->_currentGeneration);

  //justTesting();

  /* E1: Sample latent variable values */
  sampleLatent();
  _k->_logger->logInfo("Detailed", "Sampled generation: %d \n", _k->_currentGeneration);

  /* E2: Update posterior probability function Q */
  updateS();

  /* M:  Find argmax Q(theta), analytically */
  updateHyperparameters();
  // updateCholesky(); // No

   updateProbabilities();
}

/* @brief This is always run before (re-)starting the solver */
void korali::solver::HSAEM::initialize()
{
  if ((_k->_problem->getType() != "Bayesian/Latent/HierarchicalLatent") && (_k->_problem->getType() != "Bayesian/Latent/HierarchicalLatentLowlevel"))
    KORALI_LOG_ERROR("SAEM can only optimize problems of type 'Bayesian/Latent/HierarchicalLatent' or '.../HierarchicalLatentLowlevel' .\n");
  if (_k->_problem->getType() == "Bayesian/Latent/HierarchicalLatent")
  {
    _latentProblemWrapper = dynamic_cast<korali::problem::bayesian::latent::HierarchicalLatent *>(_k->_problem);
    _latentProblem = _latentProblemWrapper->_lowlevelProblem;
  }
  if (_k->_problem->getType() == "Bayesian/Latent/HierarchicalLatentLowlevel")
    _latentProblem = dynamic_cast<korali::problem::bayesian::latent::HierarchicalLatentLowlevel *>(_k->_problem);

  // Todo: make sure the problem is initialized before the solver
  _numberIndividuals = _latentProblem->_numberIndividuals;
  _latentSpaceDimensions = _latentProblem->_latentSpaceDimensions;
  // assert (_latentSpaceDimensions == _k->_variables.size()); // only variables for one individual, as prototypes, should be defined
  // \_ Edit: hSAEM initialization is also called when initializing the sub-problem, from within hierarchicalLatent._cpp. In that case _k has a complete list of variables.
  _mcmcNumberChains = _numberSamplesPerStep; // to simplify things, use one MCMC chain per final sample to generate.

  _univariateNormalGenerator->_mean = 0.0;

  for (size_t i = 0; i < _latentSpaceDimensions; i++)
    if (std::isfinite(_k->_variables[i]->_initialValue) == false)
      KORALI_LOG_ERROR("Initial Value of variable \'%s\' not defined (no defaults can be calculated).\n", _k->_variables[i]->_name.c_str());

  if ((_delta < 0. ) || (_delta > 1.))
     KORALI_LOG_ERROR("Value of 'delta' must be between 0 and 1. delta was: %.2f\n", _delta);
}

/* @brief Run once, before the first generation */
void korali::solver::HSAEM::setInitialConfiguration()
{
  // Resizing and clearing internal vectors
  assert (_mcmcNumberChains == _numberSamplesPerStep); // Else the function above is not called before this one, implementation error, need to switch those
  _currentSamples.resize(_mcmcNumberChains);
  for (size_t i = 0; i < _mcmcNumberChains; i++)
  {
    _currentSamples[i].resize(_numberIndividuals);
    for (size_t j = 0; j < _numberIndividuals; j++)
      _currentSamples[i][j].resize(_latentSpaceDimensions);
  }

  _currentSampleLogLikelihoods.resize(_mcmcNumberChains);
  _currentSampleLogPriors.resize(_mcmcNumberChains);
  for (size_t i = 0; i < _mcmcNumberChains; i++)
  {
    _currentSampleLogLikelihoods[i].resize(_numberIndividuals);
    _currentSampleLogPriors[i].resize(_numberIndividuals);
  }
  _acceptanceRate.resize(_latentSpaceDimensions);
  _acceptanceRateNominator.resize(_latentSpaceDimensions);
  _acceptanceRateDenominator.resize(_latentSpaceDimensions);
  std::fill(_acceptanceRateNominator.begin(), _acceptanceRateNominator.end(), 0.0);
  std::fill(_acceptanceRateDenominator.begin(), _acceptanceRateDenominator.end(), 0.0);
  _currentSampleStandardDeviations.resize(_numberIndividuals);
  _currentSampleMeans.resize(_numberIndividuals);
  for(size_t i=0; i < _numberIndividuals; i++){
    _currentSampleStandardDeviations[i].resize(_latentSpaceDimensions);
    _currentSampleMeans[i].resize(_latentSpaceDimensions);
  }
  _currentS1.resize(_latentSpaceDimensions);
  _currentS2.resize(_latentSpaceDimensions);
  for (size_t i = 0; i < _latentSpaceDimensions; i++)
    _currentS2[i].resize(_latentSpaceDimensions);
  std::fill(_currentS1.begin(), _currentS1.end(), 0);
  for (size_t i = 0; i < _latentSpaceDimensions; i++)
    std::fill(_currentS2[i].begin(), _currentS2[i].end(), 0);
//  _currentCholesky.resize(_latentSpaceDimensions);
//  for (size_t i = 0; i < _latentSpaceDimensions; i++)
//    _currentCholesky[i].resize(_latentSpaceDimensions);
  _mcmcStandardDeviations.resize(_latentSpaceDimensions);
  std::fill(_mcmcStandardDeviations.begin(), _mcmcStandardDeviations.end(), 1.0);

  _annealedCovariance.resize(_latentSpaceDimensions * _latentSpaceDimensions);
  std::fill(_annealedCovariance.begin(), _annealedCovariance.end(), 0);
  for (size_t i = 0; i < _latentSpaceDimensions; i++)
    // diagonal matrix, flattened
    _annealedCovariance[i + i * _latentSpaceDimensions] = _simulatedAnnealingInitialVariance; // "high" initial covariance values; adjust if needed

  // Set starting values for hyperparameters - just copy them over, we already set them in the problem initialization
  _currentHyperparametersMean.resize(_latentSpaceDimensions);
  _currentHyperparametersCovariance.resize(_latentSpaceDimensions);
  for (size_t i = 0; i < _latentSpaceDimensions; i++)
    _currentHyperparametersCovariance[i].resize(_latentSpaceDimensions);
  size_t hyperparam_index = 0;
  for (size_t i = 0; i < _latentSpaceDimensions; i++)
  {
    //    int idx =  _latentProblem->_hyperparametersMeanIndices[i];
    _currentHyperparametersMean[i] = _latentProblem->_hyperparametersMean[i]->_initialValue;
  }
  for (size_t i = 0; i < _latentSpaceDimensions; i++)
    for (size_t j = 0; j < _latentSpaceDimensions; j++)
    {
      //      int idx =  _latentProblem->_hyperparametersCovIndices[i][j];
      int idx = i * _latentProblem->_latentSpaceDimensions + j;
      _currentHyperparametersCovariance[i][j] = _latentProblem->_hyperparametersCovariance[idx]->_initialValue;
    }

  // Initialize the (transformed) latent variables
  updateAnnealedDistribution();
  _currentZ.resize(_mcmcNumberChains);
  for (size_t i = 0; i < _mcmcNumberChains; i++)
  {
    _currentZ[i].resize(_numberIndividuals);
    for (size_t j = 0; j < _numberIndividuals; j++)
    {
      _currentZ[i][j].resize(_latentSpaceDimensions);
      //std::vector<double> temp_something(1);
      _annealedNormalGenerator->getRandomVector(&_currentZ[i][j][0], _latentSpaceDimensions);
    }
  }

  // Initialize log-Likelihoods. Z is initialized above. Do not switch these two paragraphs.
  updateProbabilities(); // Todo: If works, remove what's commented out
//  for (size_t c_idx = 0; c_idx < _mcmcNumberChains; c_idx++)
//  {
//    korali::Sample sample;
//    sample["Sample Id"] = 2564234;
//    sample["Latent Variables"] = _currentZ[c_idx];
//    sample["Operation"] = "Evaluate logLikelihood";
//    sample["Module"] = "Problem";
//    _conduit->start(sample);
//    _conduit->wait(sample);
//    auto newLLHs = sample["Log Likelihood"].get<std::vector<double>>();
//    _currentSampleLogLikelihoods[c_idx] = newLLHs;
//  }

  if (_latentSpaceDimensions == 1)
    _n3 = 0;
  if (_latentProblem->_logitnormalLatentIndices.size() > 0.75 * (_latentSpaceDimensions))
    _n1 = 0; // For Logit-Normal variables, the first "chain", first N1 steps, are very unstable. That is
             // because the LLH is a bad indicator for logit-normals, and the first steps rely purely on LLH as acceptance criterion.

  if (_logAllSamples){
    _allSamplesThisIteration.resize(_mcmcNumberChains);
    _allLoglikelihoodsThisIteration.resize(_mcmcNumberChains);
    _allPriorsThisIteration.resize(_mcmcNumberChains);
    for (size_t c = 0; c < _mcmcNumberChains; c++){
      size_t allSteps = _mcmcOuterSteps * (_n1 + _n2 + _n3);
      _allSamplesThisIteration[c].resize(allSteps);
      _allLoglikelihoodsThisIteration[c].resize(allSteps);
      _allPriorsThisIteration[c].resize(allSteps);
      for (size_t st = 0; st < allSteps; st++)
      {
        _allSamplesThisIteration[c][st].resize(_numberIndividuals);
        _allLoglikelihoodsThisIteration[c][st].resize(_numberIndividuals);
        _allPriorsThisIteration[c][st].resize(_numberIndividuals);
        for (size_t j = 0; j < _numberIndividuals; j++)
          _allSamplesThisIteration[c][st][j].resize(_latentSpaceDimensions);
      }
    }
  }

  _bestLogLikelihood = -korali::Inf;
}

/* initial things to run to test the hierarchical latent problem class */
void korali::solver::HSAEM::justTesting()
{
  // for simple population example: Only means of the data, 1-dimensional
  size_t nIndividuals = 10;
  std::vector<std::vector<double>> latent_vars = {{1.25}, {1.25}, {1.25}, {1.}, {1.}, {1.}, {0.9}, {0.75}, {0.75}, {0.75}};
  std::vector<double> mean = {1};
  std::vector<std::vector<double>> cov(1);
  cov[0] = std::vector<double>({0.5});
  // sigma (sdev of the distribution of which the latent variable is the mean), and the data points, are
  // set by the problem - we can't access them here

  korali::Sample sample;
  sample["Sample Id"] = 2564235;
  sample["Latent Variables"] = latent_vars[0];
  sample["Data Point"] = {0.89};
  sample["Mean"] = mean;
  sample["Covariance Matrix"] = cov;
  sample["Module"] = "Problem";
  sample["Operation"] = "Evaluate LogLikelihood Single";
  sample["Sample Id"] = 1234567;

  _conduit->start(sample);
  _conduit->wait(sample);

  // * test evaluateConditionalLoglikelihood()

  double cond_llh = sample["Conditional LogLikelihood"].get<double>();
  _k->_logger->logInfo("Normal", "Test: Conditional LLLH = %f evaluated for latent variables:\n", cond_llh);
  for (size_t i = 0; i < _latentProblem->_latentVariableIndices.size(); i++)
  {
    int idx = _latentProblem->_latentVariableIndices[i];
    _k->_logger->logInfo("Normal", "Test: \t %s : %e \n", _latentProblem->_k->_variables[idx]->_name.c_str(), latent_vars[i]);
  }

  // * test evaluateLoglikelihood()
  sample["Latent Variables"] = latent_vars;
  sample["Operation"] = "Evaluate logPosterior";
  _conduit->start(sample);
  _conduit->wait(sample);
  double llh = sample["Log Posterior"].get<double>();
  _k->_logger->logInfo("Normal", "Test: LLH = %f evaluated for latent variables as above, plus mean:\n", llh);
  for (size_t i = 0; i < mean.size(); i++)
  {
    _k->_logger->logInfo("Normal", "Test: \t Mean %d: %e \n", i, mean[i]);
  }
  for (size_t i = 0; i < cov.size(); i++)
  {
    for (size_t j = 0; j < cov[0].size(); j++)
    {
      _k->_logger->logInfo("Normal", "Test: \t Cov[%d, %d]: %e \n", i, j, cov[i][j]);
    }
  }
}

void korali::solver::HSAEM::sampleLatent()
{
  updateAnnealedDistribution(); // because we want to draw from it below

  std::fill(_acceptanceRateDenominator.begin(), _acceptanceRateDenominator.end(), 0.0);
  std::fill(_acceptanceRateNominator.begin(), _acceptanceRateNominator.end(), 0.0);

  // only needed if we log all samples
  std::vector<std::vector<std::vector<double>>> allSamples;
  std::vector<std::vector<double>> allLLHs;
  std::vector<std::vector<double>> allPriors;
  size_t counter;

  // int NStepsPerChain = _mcmcOuterSteps * (_n1 + _n2 + _n3);
  //  std::vector<std::vector<std::vector<std::vector<double>>>> allSamples;    // nChains x nSamples x nIndividuals x nDimensions
  //allSamples.clear();
  //  std::vector<std::vector<double>> chainLogLikelihoods(_mcmcNumberChains); // nChains x nIndividuals, not sure if needed.
  //  std::vector<std::vector<double>> initialChainLogLikelihoods(_mcmcNumberChains); // nChains x nIndividuals

  //
  //  volatile int done = 0;
  //  while (!done) sleep(1);

  // ** Loop over chains
  for (size_t c_idx = 0; c_idx < _mcmcNumberChains; c_idx++)
  {
    //std::vector<std::vector<std::vector<double>>> chainSamples; -- these are stored in _currentZ
    //
    std::vector<double> currentLLHs = _currentSampleLogLikelihoods[c_idx];
    std::vector<double> currentPriors(0); // will be used in steps 2 and 3
    std::vector<double> acceptanceRates(0);
    // Access _currentZ only twice for each chain, and hope that this allows parallelism - use chainZ in between:
    std::vector<std::vector<double>> chainZ = _currentZ[c_idx];
    if (_logAllSamples){
      allSamples.resize(_mcmcOuterSteps * (_n1 + _n2 + _n3));
      allLLHs.resize(_mcmcOuterSteps * (_n1 + _n2 + _n3));
      allPriors.resize(_mcmcOuterSteps * (_n1 + _n2 + _n3));
      for (size_t st=0; st < _mcmcOuterSteps * (_n1 + _n2 + _n3); st++){
        allSamples[st].resize(_numberIndividuals);
        allLLHs[st].resize(_numberIndividuals);
        allPriors[st].resize(_numberIndividuals);
        for (size_t i=0; i < _numberIndividuals; i++)
          allSamples[st][i].resize(_latentSpaceDimensions);
      }
      counter = 0;
    }

    korali::Sample sample;
    sample["Sample Id"] = 2564236;
    sample["Module"] = "Problem";

    // ** For each chain, the zeta values to restart from are: _currentZ
    //    - wait, then we have multiple _currentZ's, one for each chain? At least? So nrChains == nrMCMCSamples?

    // ** Chain initialization
    std::vector<std::vector<double>> eta(_numberIndividuals); // NrIndividuals x NrDimensions
    for (size_t indiv = 0; indiv < _numberIndividuals; indiv++)
    {
      eta[indiv].resize(_latentSpaceDimensions);
      // current eta = current z - current mean
      std::transform(chainZ[indiv].begin(), chainZ[indiv].end(), _currentHyperparametersMean.begin(), eta[indiv].begin(), std::minus<double>());
    }

    // ** Loop over one chain
    for (size_t step = 0; step < _mcmcOuterSteps; step++)
    {
      //std::vector<std::vector<double>> stepSamples;  // NrIndividuals x NrDimensions
      //stepSamples.clear();


      // *****************************
      // ** 1st proposal distribution:
      // *****************************
      //    Sample directly according to p(latent | hyperparameter), ignoring the data. Accept according to the ratios
      //       of the data likelihoods.
      for (size_t i = 0; i < _n1; i++)
      {
        // Proposals q1: Drawn from p(latent | hyperparam)
        std::vector<std::vector<double>> newZ(_numberIndividuals); // NrIndividuals x NrDimensions
        std::vector<double> tempZ(_latentSpaceDimensions);
        for (size_t indiv = 0; indiv < _numberIndividuals; indiv++)
        {
          _annealedNormalGenerator->getRandomVector(&tempZ[0], _latentSpaceDimensions); // == Previous samples minus par.beta --Why?
          newZ[indiv] = tempZ;                                                       // TODO: Check that this is not copy-by-reference or something
        }

        // generate acceptance probabilities: new llh / former llh
        sample["Latent Variables"] = newZ; // Note: We need not transform to non-Z form; the function called with "Evaluate logLikelihood" expects Z form
        sample["Operation"] = "Evaluate logLikelihood";
        _conduit->start(sample);
        _conduit->wait(sample);
        auto newLLHs = sample["Log Likelihood"].get<std::vector<double>>();
        for (size_t indiv = 0; indiv < _numberIndividuals; indiv++)
        {
          // generate RVs to randomly accept or reject each new zeta
          double logAcceptanceProb = newLLHs[indiv] - currentLLHs[indiv];
          double rval = _uniformGenerator->getRandomNumber();
          if (log(rval) < logAcceptanceProb)
          {
            // update zeta, update likelihood
            chainZ[indiv] = newZ[indiv];
            // Todo: Do not write to currentZ directly here, instead create a temporary array and write back everything
            //             only after the loop over all chains? I don't know enough about the optimization / what will be
            //             parallelized in both cases. It could help.
            // Todo: Look at (T)MCMC, how are separate chains managed there?
            currentLLHs[indiv] = newLLHs[indiv];
          }
        }
        if (_logAllSamples){
          allSamples[counter] = chainZ;
          allLLHs[counter] = currentLLHs;
          allPriors[counter] = std::vector<double>(_numberIndividuals, 0); // we don't track priors here
          counter++;
        }
      }

      // initialize log priors
      sample["Latent Variables"] = chainZ;
      sample["Mean"] = _currentHyperparametersMean;
      sample["Covariance Matrix"] = _currentHyperparametersCovariance;
      sample["Operation"] = "Evaluate logPrior";
      _conduit->start(sample);
      _conduit->wait(sample);
      currentPriors = sample["Log Prior"].get<std::vector<double>>();


      // *****************************
      // ** 2nd proposal distribution
      // *****************************
      //    sample each coordinate separately, according to a normal distribution unrelated to any variable
      //    --> the acceptance probability is then a function of latent variables and hyperparameters
      for (size_t i = 0; i < _n2; i++)
      {
        for (size_t dim = 0; dim < _latentSpaceDimensions; dim++)
        {
          // we will use this to sample, and then update _mcmcStandardDeviations after step 2, and then again after step 3
          _univariateNormalGenerator->_standardDeviation = _mcmcStandardDeviations[dim];
          _univariateNormalGenerator->updateDistribution();
//          assert(_univariateNormalGenerator->_standardDeviation != 1.);

          std::vector<std::vector<double>> localZ = chainZ;
          for (size_t indiv = 0; indiv < _numberIndividuals; indiv++)
          {
            double eta = _univariateNormalGenerator->getRandomNumber(); // Todo: Adjust covariance below - check whether we count acceptance for each coordinate or how
            // Adjust eta, eta is ~ N(0, 1) and we need other sdev
//            eta *= _mcmcStandardDeviations[dim];
            localZ[indiv][dim] += eta;
          }

          sample["Latent Variables"] = localZ;
          sample["Operation"] = "Evaluate logLikelihood";
          _conduit->start(sample);
          _conduit->wait(sample);
          auto newLLHs = sample["Log Likelihood"].get<std::vector<double>>();

          sample["Latent Variables"] = localZ;
          sample["Mean"] = _currentHyperparametersMean;
          sample["Covariance Matrix"] = _currentHyperparametersCovariance;
          sample["Operation"] = "Evaluate logPrior";
          _conduit->start(sample);
          _conduit->wait(sample);
          auto newPriors = sample["Log Prior"].get<std::vector<double>>();

          // acceptance probability == ratio of new prior * log to the current one
          for (size_t indiv = 0; indiv < _numberIndividuals; indiv++)
          {
            double logAcceptanceProb = newLLHs[indiv] + newPriors[indiv] - currentLLHs[indiv] - currentPriors[indiv];
            double rval = _uniformGenerator->getRandomNumber();
            if (log(rval) < logAcceptanceProb)
            {
              chainZ[indiv] = localZ[indiv];
              _acceptanceRateNominator[dim] += 1.0;
              currentLLHs[indiv] = newLLHs[indiv];
              currentPriors[indiv] = newPriors[indiv];
            }
          }
          _acceptanceRateDenominator[dim] += _numberIndividuals;
        }
        if (_logAllSamples){
          allSamples[counter] = chainZ;
          allLLHs[counter] = currentLLHs;
          allPriors[counter] = currentPriors;
          counter++;
        }
      }

      // **  update variances
      if (_n2 > 0)
      {
        for (size_t dim = 0; dim < _latentSpaceDimensions; dim++)
        {
          _acceptanceRate[dim] = ( _acceptanceRateNominator[dim]) /( _acceptanceRateDenominator[dim]);
          _mcmcStandardDeviations[dim] *= (1.0 + _delta * (_acceptanceRate[dim] - _mcmcTargetAcceptanceRate));
          assert(_mcmcStandardDeviations[dim] > 0.);
        }
//        // ** reset acceptance rate tracker to zero
//        std::fill(_acceptanceRateDenominator.begin(), _acceptanceRateDenominator.end(), 0.0);
//        std::fill(_acceptanceRateNominator.begin(), _acceptanceRateNominator.end(), 0.0);
      }

      // *****************************
      // ** 3rd proposal distribution:
      //    - Sample random subsets of coordinates jointly
      // *****************************
      for (size_t i = 0; i < _n3; i++)
      {
        // pick a number of coordinates to change simultaneously
        double nRandCoords_d = _uniformGenerator->getRandomNumber() * double(_latentSpaceDimensions) + 1.;
        if (size_t(nRandCoords_d) == _latentSpaceDimensions + 1) // Todo: Check the cast work
          nRandCoords_d = 1.;
        size_t nRandCoords = size_t(nRandCoords_d); // Todo: Check the cast work

        std::vector<size_t> randomCoords(_latentSpaceDimensions);
        std::iota(randomCoords.begin(), randomCoords.end(), 0);
        std::random_shuffle(randomCoords.begin(), randomCoords.end());
        assert (nRandCoords <= randomCoords.size());
        randomCoords.resize(nRandCoords);

        std::vector<std::vector<double>> localZ = chainZ;
        for (size_t dim : randomCoords){
          _univariateNormalGenerator->_standardDeviation = _mcmcStandardDeviations[dim];
          _univariateNormalGenerator->updateDistribution();
//          assert(_univariateNormalGenerator->_standardDeviation != 1.);

          for (size_t indiv=0; indiv < _numberIndividuals; indiv++){
            double eta = _univariateNormalGenerator->getRandomNumber();
            // Adjust eta, eta is ~ N(0, 1) and we need other sdev
//            eta *= _mcmcStandardDeviations[dim];
            localZ[indiv][dim] += eta;
          }
        }
        // Evaluate localZ proposals for each individual
        sample["Latent Variables"] = localZ;
        sample["Operation"] = "Evaluate logLikelihood";
        _conduit->start(sample);
        _conduit->wait(sample);
        auto newLLHs = sample["Log Likelihood"].get<std::vector<double>>();

        sample["Latent Variables"] = localZ;
        sample["Mean"] = _currentHyperparametersMean;
        sample["Covariance Matrix"] = _currentHyperparametersCovariance;
        sample["Operation"] = "Evaluate logPrior";
        _conduit->start(sample);
        _conduit->wait(sample);
        auto newPriors = sample["Log Prior"].get<std::vector<double>>();

        // Accept? -- Same acceptance calculation as in 2nd proposal distribution.
        for (size_t indiv = 0; indiv < _numberIndividuals; indiv++)
        {
            double logAcceptanceProb = newLLHs[indiv] + newPriors[indiv] - currentLLHs[indiv] - currentPriors[indiv];
            double rval = _uniformGenerator->getRandomNumber();
            if (log(rval) < logAcceptanceProb)
            {
              chainZ[indiv] = localZ[indiv];
              currentLLHs[indiv] = newLLHs[indiv];
              currentPriors[indiv] = newPriors[indiv];
              for (size_t dim : randomCoords){
                _acceptanceRateNominator[dim] += 1.0;
              }
            }
            for (size_t dim : randomCoords)
              _acceptanceRateDenominator[dim] += 1.0;
        }
        if (_logAllSamples){
          allSamples[counter] = chainZ;
          allLLHs[counter] = currentLLHs;
          allPriors[counter] = currentPriors;
          counter++;
        }
      }
      // **  update variances
      if (_n3 > 0)
        for (size_t dim = 0; dim < _latentSpaceDimensions; dim++)
        {
          if (_acceptanceRateDenominator[dim] == 0)
            continue; // This can happen if a coordinate is never selected as part of any random set
          _acceptanceRate[dim] = (_acceptanceRateNominator[dim]) /( _acceptanceRateDenominator[dim]);
          _mcmcStandardDeviations[dim] *= (1.0 + _delta * (_acceptanceRate[dim] - _mcmcTargetAcceptanceRate));
          assert(_mcmcStandardDeviations[dim] > 0.);
        }
//        // ** reset acceptance rate tracker to zero
//        std::fill(_acceptanceRateDenominator.begin(), _acceptanceRateDenominator.end(), 0.0);
//        std::fill(_acceptanceRateNominator.begin(), _acceptanceRateNominator.end(), 0.0);


    } // Chain loop end

    _currentSampleLogLikelihoods[c_idx] = currentLLHs;
    _currentSampleLogPriors[c_idx] = currentPriors;

    _currentZ[c_idx] = chainZ;
    for (size_t indiv=0; indiv < _numberIndividuals; indiv++){
      auto dbg_var = chainZ[indiv];
      _currentSamples[c_idx][indiv] = _latentProblem->zToLatent(chainZ[indiv]);
      }

    if (_logAllSamples){
      _allPriorsThisIteration[c_idx] = allPriors;
      _allLoglikelihoodsThisIteration[c_idx] = allLLHs;
      _allSamplesThisIteration[c_idx] = allSamples;
    }

  } // Loop over all chains end


   // mean and variance:
  auto transposed3D = transpose3D(_currentSamples);
  for (size_t indiv = 0; indiv < _numberIndividuals; indiv++){
    auto transposed2D = transposed3D[indiv];
    assert (transposed2D.size() == _latentSpaceDimensions);
    _currentSampleMeans[indiv] = std::vector<double>(transposed2D.size(), 0.0);
    _currentSampleStandardDeviations[indiv] = std::vector<double>(transposed2D.size(), 0.0);

    for (size_t i = 0; i < transposed2D.size(); i++)
    {
      std::vector<double> mean_and_sdev = meanAndSDev(transposed2D[i]);
      _currentSampleMeans[indiv][i] = mean_and_sdev[0];
      _currentSampleStandardDeviations[indiv][i] = mean_and_sdev[1];
    }
  }
  _currentMeanLogProbability = 0;
  for (size_t c_idx = 0; c_idx < _mcmcNumberChains; c_idx++){
    _currentMeanLogProbability = std::accumulate(_currentSampleLogLikelihoods[c_idx].begin(),
                                                        _currentSampleLogLikelihoods[c_idx].end(), _currentMeanLogProbability);
    _currentMeanLogProbability = std::accumulate(_currentSampleLogPriors[c_idx].begin(),
                                                        _currentSampleLogPriors[c_idx].end(), _currentMeanLogProbability);
  }
  _currentMeanLogProbability /= double(_mcmcNumberChains);

}

//* @brief Update internal distribution representation of p(latent | hyper), and anneal the covariance if needed.
void korali::solver::HSAEM::updateAnnealedDistribution()
{
  if (_useSimulatedAnnealing)
  {
    auto flatCov = flatten(_currentHyperparametersCovariance);
    for (size_t i = 0; i < flatCov.size(); i++)
      _annealedCovariance[i] = std::max(_annealedCovariance[i] * _simulatedAnnealingDecayFactor, flatCov[i]);
    _annealedNormalGenerator->setProperty("Sigma", _annealedCovariance);
  }
  else
    _annealedNormalGenerator->setProperty("Sigma", flatten(_currentHyperparametersCovariance));
  _annealedNormalGenerator->setProperty("Mean Vector", _currentHyperparametersMean);
  _annealedNormalGenerator->updateDistribution();
}

void korali::solver::HSAEM::updateS()
{
  // Determine alpha
  double alpha;
  if (_k->_currentGeneration > _numberInitialSteps)
    alpha = _alpha2;
  else
    alpha = _alpha1;

  // --> decay factor gamma
  double curGen = double(_k->_currentGeneration);
  if (curGen <= _k1)
    _gamma = 1.0;
  else
    _gamma = std::pow(curGen, -alpha);

  // * update S1
  for (size_t dim = 0; dim < _latentSpaceDimensions; dim++)
  {
    double sumZ = 0;
    for (size_t chain = 0; chain < _mcmcNumberChains; chain++)
      for (size_t indiv = 0; indiv < _numberIndividuals; indiv++)
        sumZ += _currentZ[chain][indiv][dim];
    _currentS1[dim] = _currentS1[dim] + _gamma * (sumZ / double(_mcmcNumberChains) - _currentS1[dim]);
  }
  // * update S2
  for (size_t i = 0; i < _latentSpaceDimensions; i++)
  {
    for (size_t j = 0; j < _latentSpaceDimensions; j++)
    {
      double sumZiZj = 0;
      for (size_t chain = 0; chain < _mcmcNumberChains; chain++)
        for (size_t indiv = 0; indiv < _numberIndividuals; indiv++)
          sumZiZj += _currentZ[chain][indiv][i] * _currentZ[chain][indiv][j];
      _currentS2[i][j] = _currentS2[i][j] + _gamma * (sumZiZj / double(_mcmcNumberChains) - _currentS2[i][j]);
    }
  }
}

void korali::solver::HSAEM::updateHyperparameters()
{
  for (size_t i = 0; i < _latentSpaceDimensions; i++)
    _currentHyperparametersMean[i] = _currentS1[i] / double(_numberIndividuals);
  for (size_t i = 0; i < _latentSpaceDimensions; i++)
    for (size_t j = 0; j < _latentSpaceDimensions; j++)
      _currentHyperparametersCovariance[i][j] = _currentS2[i][j] / double(_numberIndividuals)
                                                    - _currentHyperparametersMean[i] * _currentHyperparametersMean[j];
  // No simulated annealing here, that is only done with the 'annealed normal distribution' used in sampling.
}

/* Call after updating hyperparameters: Re-calculate the likelihood of all current samples.
   Need to be already set up: _currentHyperparametersMean and _currentHyperparametersCovariance and _currentZ
*/
void korali::solver::HSAEM::updateProbabilities(){
  for (size_t c_idx = 0; c_idx < _mcmcNumberChains; c_idx++)
  {
    korali::Sample sample;
    sample["Sample Id"] = 2564242;
    sample["Latent Variables"] = _currentZ[c_idx];
    sample["Operation"] = "Evaluate logLikelihood";
    sample["Module"] = "Problem";
    _conduit->start(sample);
    _conduit->wait(sample);
    auto newLLHs = sample["Log Likelihood"].get<std::vector<double>>();
    _currentSampleLogLikelihoods[c_idx] = newLLHs;

    sample["Operation"] = "Evaluate logPrior";
    sample["Latent Variables"] = _currentZ[c_idx];
    sample["Mean"] = _currentHyperparametersMean;
    sample["Covariance Matrix"] = _currentHyperparametersCovariance;
    _conduit->start(sample);
    _conduit->wait(sample);
    auto newPriors = sample["Log Prior"].get<std::vector<double>>();
    _currentSampleLogPriors[c_idx] = newPriors;


  }
  double LLHSum = 0;
  for (size_t c_idx = 0; c_idx <_mcmcNumberChains; c_idx++){
    LLHSum = std::accumulate(_currentSampleLogLikelihoods[c_idx].begin(), _currentSampleLogLikelihoods[c_idx].end(), LLHSum);
    LLHSum = std::accumulate(_currentSampleLogPriors[c_idx].begin(), _currentSampleLogPriors[c_idx].end(), LLHSum);
  }
  _currentLogLikelihood = LLHSum / double(_mcmcNumberChains);
  if (_currentLogLikelihood > _bestLogLikelihood)
    _bestLogLikelihood = _currentLogLikelihood;

}


/** @brief Utility function to calculate mean and standard deviation of the values in vector v. */
std::vector<double> korali::solver::HSAEM::meanAndSDev(std::vector<double> v)
{
  // Todo: To save space and not duplicate code, use gsl_stats_mean() and gsl_stats_sd_m() instead.

  // Origin: https://stackoverflow.com/questions/7616511/calculate-mean-and-standard-deviation-from-a-vector-of-samples-in-c-using-boos
  double sum = std::accumulate(v.begin(), v.end(), 0.0);
  double mean = sum / double(v.size());

  std::vector<double> diff(v.size());
  std::transform(v.begin(), v.end(), diff.begin(), [mean](double x) { return x - mean; });
  double sq_sum = std::inner_product(diff.begin(), diff.end(), diff.begin(), 0.0);
  double stdev = std::sqrt(sq_sum / double(v.size()));
  std::vector<double> result = {mean, stdev};
  return result;
}

/** @brief Utility function, "transposes" a vector of vectors of vectors @param data, so that
             data[i][j][k] will be moved to result[j][k][i].
    */
std::vector<std::vector<std::vector<double>>> korali::solver::HSAEM::transpose3D(const std::vector<std::vector<std::vector<double>>> data)
{
  /* From: https://stackoverflow.com/questions/6009782/how-to-pivot-a-vector-of-vectors */
  // this assumes that all inner vectors have the same size
  std::vector<std::vector<std::vector<double>>> result(data[0].size(), std::vector<std::vector<double>>(data[0][0].size(),
                                                                              std::vector<double>(data.size())));
  for (std::vector<double>::size_type j = 0; j < data[0].size(); j++)
    for (std::vector<double>::size_type k = 0; k < data[0][0].size(); k++)
      for (std::vector<double>::size_type i = 0; i < data.size(); i++)
        result[j][k][i] = data[i][j][k];
  return result;
}



void korali::solver::HSAEM::printGenerationBefore()
{
  _k->_logger->logInfo("Normal", "Preparing to start generation...\n");
}

void korali::solver::HSAEM::printGenerationAfter()
{
  _k->_logger->logInfo("Normal", "Finished generation %lu...\n", _k->_currentGeneration);

  _k->_logger->logInfo("Normal", "    Current Total LogLikelihood:          %.2e\n", _currentLogLikelihood);
  _k->_logger->logInfo("Normal", "    Best Total LogLikelihood:             %.2e\n", _bestLogLikelihood);
  // ** Show current hyperparameters
  _k->_logger->logInfo("Normal", " Current hyperparameters, mean: \n");
  for (size_t dim = 0; dim < _latentSpaceDimensions; dim++){
    size_t idx = _latentProblem->_firstIndividualLatentIndices[dim];
    std::string varName = _latentProblem->_k->_variables[idx]->_name;
    _k->_logger->logInfo("Normal", "\tMean of %s etc:\n", varName.c_str());
    _k->_logger->logInfo("Normal", "\t\t%.2f  \n",_currentHyperparametersMean[dim]);
  }
  std::ostringstream covLogStringStream;
  _k->_logger->logInfo("Normal", "Current hyperparameters, covariance matrix: \n");
  covLogStringStream << "  [";
//  _k->_logger->logInfo("Normal",   "  [");
  for (size_t i = 0; i < _latentSpaceDimensions; i++)
  {
     covLogStringStream <<  "    [";
     for (size_t j = 0; j < _latentSpaceDimensions; j++)
       covLogStringStream << "\t" << _currentHyperparametersCovariance[i][j] << " ";
     covLogStringStream << "],\n";
     _k->_logger->logInfo("Normal", covLogStringStream.str().c_str());
     covLogStringStream.str("");
  }
  _k->_logger->logInfo("Normal",   "    ]\n");
  // ** Print current sampled values
  for (size_t c_idx = 0; c_idx < _mcmcNumberChains; c_idx++)
  {                             // TODO: Report the non-transformed latents instead. More informative.
      _k->_logger->logInfo("Detailed", "    - Current latent variable samples of chain %d: \n", c_idx);
      for (size_t i = 0; i < _numberIndividuals; i++)
      {
        _k->_logger->logInfo("Detailed", "      - Individual %d: \n", i);
        for (size_t dim = 0; dim < _latentSpaceDimensions; dim++)
        {
          int idx = _latentProblem->_latentIndex[i][dim];
          _k->_logger->logInfo("Detailed", "        %s : %.2f \n", _latentProblem->_k->_variables[idx]->_name.c_str(),
                                                                                            _currentSamples[c_idx][i][dim]);
        }
      }
      for (size_t i = 0; i < _numberIndividuals; i++)
      { // ** In modus "detailed", also print llhs and acceptance
        _k->_logger->logInfo("Detailed", "      - Individual %d: \n", i);
        _k->_logger->logInfo("Detailed", "          LLH:       %.2f \n", _currentSampleLogLikelihoods[c_idx][i]);
        _k->_logger->logInfo("Detailed", "          Log-Prior: %.2f: \n", _currentSampleLogPriors[c_idx][i]);
      }
      _k->_logger->logInfo("Detailed", "    - Acceptance rates, chain %d: \n", c_idx);
      for (size_t dim = 0; dim < _latentSpaceDimensions; dim++)
        _k->_logger->logInfo("Detailed", "      Dim %d: %.3f \n", dim, _acceptanceRate[dim]);
  }

  //* Show current mean log-likelihood across samples -- Todo: Need to update these.
}


//void finalize(){
//
//}

/** @brief: Utility function to convert a vector of vectors into a concatenated 1D vector.
  */
std::vector<double> korali::solver::HSAEM::flatten(const std::vector<std::vector<double>> &v)
{
  // credits to Sebastian Redl @ stackoverflow
  std::size_t total_size = 0;
  for (const auto &sub : v)
    total_size += sub.size();
  std::vector<double> result;
  result.reserve(total_size);
  for (const auto &sub : v)
    result.insert(result.end(), sub.begin(), sub.end());
  return result;
}
